\chapter{Details of training}
\label{app: training}
In order to train all models in this thesis, I use the Adam \cite{kingma2014adam} optimizer with a starting learning rate of 0.0006 for training on the Pascal1k and Flickr8k datasets, and a starting learning rate of 0.0004 for training on the Flickr30k dataset. During training, the learning rate was halved down every 5 epochs for the smaller Pascal1k and Flickr8k datasets, and every 10 epochs for the larger Flickr30k dataset. A margin of 0.2 is used for training a \textit{Siamese multi-hop attention} model on all three datasets. The number of neurons of the hidden layer for the \textit{multi-hop attention} is set to 256 for the Pascal1k and Flickr8k dataset, and 64 neurons were used for the Flickr30k dataset. In addtion, 30 attention hops were used for training a model on the Pascal1k and Flicrk8k datasets, and 10 attention hops for the Flickr30k dataset. The size of the joint space where both modalities are projected was set to 1024 and the gradients where clipped when the global norm exceeded 2.0 for all three datasets. Moreover, an L2 weight decay of 0.0001 was used to train all models.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
