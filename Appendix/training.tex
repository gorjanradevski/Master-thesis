\chapter{Details of training}
\label{app: training}
The whole end-to-end deep learning architecture is implemented using the library Tensorflow \cite{abadi2016tensorflow}. Moreover, to come up with the best hyperparameter combination for each of the datasets, hyperparameter tuning is performed with the library Hyperopt \cite{bergstra2015hyperopt}.\endgraf
To train the models in this thesis, I use the Adam \cite{kingma2014adam} optimizer with a starting learning rate of 0.0006 for training on the \textit{Pascal1k} and \textit{Flickr8k} datasets, and a starting learning rate of 0.0004 for training on the \textit{Flickr30k} dataset. During training, the learning rate is halved down every 5 epochs for the smaller \textit{Pascal1k} and \textit{Flickr8k} datasets and every 10 epochs for the larger \textit{Flickr30k} dataset. A margin of 0.2 is used for training a \textit{Siamese multi-hop attention} model on all three datasets. The number of neurons of the hidden layer for the multi-hop attention is set to 256 for the \textit{Pascal1k} and \textit{Flickr8k} dataset, and 64 neurons are used for the \textit{Flickr30k} dataset. In addition, 30 attention hops are used for training a model on the Pascal1k and \textit{Flicrk8k} datasets, and 10 attention hops for the \textit{Flickr30k} dataset. The size of the joint space where both modalities are projected is set to 1024 and the gradients are clipped when the global norm exceeds 2.0 for all three datasets. The penalization term is used as it is, meaning that is not scaled by a constant. Moreover, an L2 weight decay of 0.0001 is used in addition to the matching loss and the penalization term to contribute to the total model loss.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
