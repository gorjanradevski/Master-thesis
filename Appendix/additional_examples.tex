\chapter{Additional examples}
\label{additional_examples}
In this section I present additional examples of cross-modal retrieval on the image-text and text-image cases. In particular, I showcase the query modality as well as the top 5 retrieved samples from the opposing modality. Moreover, for each of the modalities the attention weights are visualized.

\begin{figure}
  \centering
  \includegraphics[width=130mm]{Images/image-text-pascal.pdf}
  \caption[Image-text retrieval \textit{Pascal1k}]{Visualizing the image-text retrieval on the \textit{Pascal1k} dataset. The image on the left is the query image while the images on the right are the attention weights from two distinct attention hops. The 5 sentences below the images are the retrieved sentences. The red colored words indicates increased attention weights on those words.}
  \label{fig:image-text-pascal}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=120mm]{Images/text-image-pascal.pdf}
  \caption[Text-image retrieval \textit{Pascal1k}]{Visualizing the text-image retrieval on the \textit{Pascal1k} dataset. The image on the left is the query image while the images on the right are the attention weights from two distinct attention hops. The 5 sentences below the images are the retrieved sentences. The red colored words indicates increased attention weights on those words.}
  \label{fig:text-image-pascal}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=130mm]{Images/image-text-flicrk8k.pdf}
  \caption[Image-text retrieval \textit{Flickr8k}]{Visualizing the image-text retrieval case on the \textit{Flickr8k} dataset. The image on the left is the query image while the images on the right are the attention weights from two distinct attention hops. The 5 sentences below the images are the retrieved sentences. The red colored words indicates increased attention weights on those words.}
  \label{fig:image-text-flickr8k}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=120mm]{Images/text-image-flickr8k.pdf}
  \caption[Text-image retrieval \textit{Flickr8k}]{Visualizing the text-image retrieval on the \textit{Flickr8k} dataset. The image on the left is the query image while the images on the right are the attention weights from two distinct attention hops. The 5 sentences below the images are the retrieved sentences. The red colored words indicates increased attention weights on those words.}
  \label{fig:text-image-flickr8k}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=130mm]{Images/image-text-flickr30k.pdf}
  \caption[Image-text retrieval \textit{Flickr30k}]{Visualizing the image-text retrieval case on the \textit{Flickr30k} dataset. The image on the left is the query image while the images on the right are the attention weights from two distinct attention hops. The 5 sentences below the images are the retrieved sentences. The red colored words indicates increased attention weights on those words.}
  \label{fig:image-text-flickr30k}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=120mm]{Images/text-image-flickr30k.pdf}
  \caption[Text-image retrieval \textit{Flickr30k}]{Visualizing the text-image retrieval on the \textit{Flickr30k} dataset. The image on the left is the query image while the images on the right are the attention weights from two distinct attention hops. The 5 sentences below the images are the retrieved sentences. The red colored words indicates increased attention weights on those words.}
  \label{fig:text-image-flickr30k}
\end{figure}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
