\chapter{Introduction}
\label{cha:intro}

\section{Image-text matching}
In this thesis, the problem of image-text matching, an area vital to cross-modal retrieval, is studied. Cross-modal retrieval is a task where we attempt to retrieve contents from one modality \textit{A}, given a query from another modality \textit{B}, where $A \neq B$. The crucial part of being able to retrieve contents from one modality, given a query from another modality, is to infer which \textit{A} modal contents match the \textit{B} modal query. Therefore, having a method that can successfully do cross-modal matching leads to the development of cross-modal retrieval systems. These systems relax the constraint of processing user queries only from the same modality as the database entries.
\endgraf
Image-text matching plays an essential role for sentence retrieval (retrieving sentences given an image query) and image retrieval (retrieving images given a sentence query) \cite{kiros2014unifying, frome2013devise, ma2015multimodal, mao2014explain, klein2015associating, faghri2017vse++, wang2018learning, socher2014grounded, nam2017dual, lee2018stacked, karpathy2014deep, karpathy2015deep, huang2017instance}. Recently, a variety of methods that attempt to solve the image-text matching problem have been proposed. The methods generally fall into two categories:

\begin{itemize}
    \item One-to-one matching \cite{kiros2014unifying, frome2013devise, ma2015multimodal, mao2014explain, klein2015associating, faghri2017vse++, wang2018learning, socher2014grounded}: Methods where a global representation is extracted from both modalities and matched using a similarity measure.
    \item Many-to-many matching \cite{nam2017dual, lee2018stacked, karpathy2014deep, karpathy2015deep, huang2017instance}: Methods where a sequence of local representations are extracted from both modalities, and the global similarity is computed as an average of the local similarities.
\end{itemize}

In this thesis, I argue that the methodology employed by the one-to-one matching methods is not sophisticated enough to consider all alignments between an image and a sentence. On the other hand, the many-to-many methods take upon an approach which amplifies the complexity of the image-text matching problem.\endgraf
Finally, the \textit{Siamese multi-hop attention} model tries to bridge the gap between the one-to-one and the many-to-many matching methods. The modus operandi of \textit{SMHA} is to leverage the simplicity of the one-to-one matching methods while taking advantage of the comprehensive methodology the many-to-many matching methods employ. To do so, \textit{SMHA} employs a multi-step process in an iterative manner, where a global representation is extracted from both modalities multiple times. Each time the representations are extracted, \textit{SMHA} focuses on different aspects of the modalities, thus searching for all possible relations that may exist between the image and the text. 

\section{Attention mechanism}

The essential component of the \textit{Siamese multi-hop attention} is the attention mechanism \cite{bahdanau2014neural, xu2015show}. Earlier variants of the attention mechanism were applied in neural machine translation \cite{bahdanau2014neural} and in neural caption generation \cite{xu2015show}, where attention is applied to associate two sources of information. Later, \citet{cheng2016long, parikh2016decomposable, yang2016hierarchical} implemented self-attention, an attention variant used in conjunction with a recurrent neural network to compute a single linear combination of the hidden states, and thus represent the sequence. \citet{lin2017structured} argue that computing a single linear combination is not sufficient to capture all relations between the entities in the sequence, hence they introduce structured self-attention. With the structured self-attention, \citet{lin2017structured} extend the single-hop self-attention to a multi-hop one, where multiple potentially distinct linear combinations of the hidden states are computed. In this thesis, I start from the premise that multi-hop attention is crucial for successful image-text matching. In regards to that, the contributions of this thesis are the following:
\begin{itemize}
    \item I expand the structured self-attention \cite{lin2017structured} for the image-text matching problem. I conduct a series of ablation studies to prove that multi-hop attention is necessary to find all alignments between an image and a sentence.
    \item I empirically prove that using multiple hops of attention on its own is not enough for the model to leverage the increased capacity. As a result, a penalization term \cite{lin2017structured} is implemented to enforce diversity between the attention hops, and push the model towards fully utilizing the multi-hop attention.
    \item By leveraging attention, the \textit{Siamese multi-hop attention} model is interpretable and can visualize the multi-step process it partakes to embed the modalities in the joint latent space.
\end{itemize}

\section{Siamese networks}
A siamese neural network \cite{bromley1994signature} consists of twin networks that have tied weights. The siamese neural networks were firstly used by \citet{bromley1994signature} to solve the challenge of signature verification, and now are the de facto deep neural network architecture for face verification \cite{taigman2014deepface}. Due to the tied weights, it is guaranteed that two highly similar inputs can not be embedded in different locations in the joint latent space. Moreover, the twin networks compute the same function, so even if the inputs are switched between the twin networks, the output remains the same, thus making the siamese neural network symmetrical. However, the success of the siamese neural networks has been purely related to unimodal matching, and to the best of my knowledge, no study has attempted to leverage a siamese architecture for cross-modal matching. On the other hand, the contribution of this thesis is the following:
\begin{itemize}
   \item The two branch multi-hop attention is merged in one attention module trained jointly for both modalities. Namely, the \textit{Siamese multi-hop attention} ties the attention weights for both the image and sentence pathway of the model.
   \item  An ablation study is carried out to demonstrate that the less memory demanding \textit{Siamese multi-hop attention} performs  on par with the multi-hop attention, and learns to pay attention to both modalities at once.
\end{itemize}


\section{Transfer learning}\label{sec: transfer}
Transfer learning is a method where weights from a deep learning model trained on task \textit{A}, involving a feature space $X_A$ and a label space $Y_A$, are transferred to a different deep learning model to perform on task \textit{B} with a feature space $X_B$ and a label space $Y_B$ where $A \neq B$. By doing so, the receiving model can make use of the knowledge from the model trained on task \textit{A}. Moreover, transfer learning is particularly useful when the domains of task \textit{A} and task \textit{B} are similar, as is the case for the problem being solved in this thesis.\endgraf
Due to the nature and size of the image-text matching datasets, in this thesis, I make heavy use of transfer learning. Compared to other image-text matching approaches where knowledge is transferred to the image encoder, here knowledge is transferred from pre-trained models to both the image and sentence encoder.\endgraf
To summarize, with the addition of transfer learning to the sentence encoder branch, the final contributions of this thesis are:
\begin{itemize}
    \item Compared to other methods for image-text matching, where knowledge from pre-trained models is transferred only to the image encoder branch, \textit{Siamese multi-hop attention} makes use of transfer learning on both the image encoder and the sentence encoder branch.
    \item The \textit{Siamese multi-hop attention} model, with the addition of transferred knowledge to the sentence encoder branch, demonstrates robustness and achieves results competitive to the state-of-the-art on \textit{Pascal1k}, \textit{Flickr8k}, and \textit{Flickr30k} datasets.
\end{itemize}