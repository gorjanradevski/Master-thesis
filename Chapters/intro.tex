\chapter{Introduction}
\label{cha:intro}

\section{The problem of cross-modal retrieval}
In this thesis, the problem of cross-modal retrieval in the image-text and text-image case are studied. The cross-modal retrieval in the image-text case is when the machine is presented with a query image, and it is supposed to return a ranked list of sentences that match the image from a database . On the contrary, the text-image case is when the machine is presented with a query sentence, and it is supposed to return a ranked list of images that match the sentence from a database. The complexity in the cross-modal retrieval scenario arises from the fact that the machine, in order to perform the cross-modal retrieval it should do cross-modal matching between two or more modalities which come from a different source. \endgraf
Recently, a variety of methods have been proposed to solve the cross-modal retrieval problem. The methods generally fall into two categories:
\begin{itemize}
    \item One-to-one matching \cite{kiros2014unifying, frome2013devise, ma2015multimodal, mao2014explain, klein2015associating, faghri2017vse++, wang2018learning, socher2014grounded}: Methods where a global representation is extracted from both modalities, and matched using a similarity measure.
    \item Many-to-many matching \cite{nam2017dual, lee2018stacked, karpathy2014deep, karpathy2015deep, huang2017instance}: Methods where a sequence of local representations are extracted from both modalities, and the global similarity is computed as an average of the local similarities.
\end{itemize}
In order to demystify things, let us make an analogy of how a person will do cross-modal matching when one modality is an image and the other is a sentence. Consequently, the sentence is related to the image and will weakly annotate the image according to the entities present. Therefore, a person will not have a single look at both the image and the sentence to encode their meaning into memory, and later decide how well they match as the one-to-one matching methods do. Furthermore, even though more alike, the image-text matching will not come down to encoding local parts of the image and the sentence into memory as followed by an average match between all parts the many-to-many methods do. Ultimately, the human visual attention system will attempt to encode multiple aspects of both the image and the sentences into a single global entry into memory by attending multiple times at both modalities. In that sense, the work done in this thesis tries to fill the gap between the one-to-one matching methods and the many-to-many matching methods by performing iterative multi-hop attentive process over both of the modalities. The motivation for doing so is to make the image-text matching as homogeneous as possible to the way a person would match an image and a sentence. 
\section{Attention mechanism}
The essential component of \textit{Siamese multi-hop attention} is the attention mechanism \cite{bahdanau2014neural, xu2015show}. The earlier variants of the attention mechanism applied in the neural machine translation \cite{bahdanau2014neural} and in neural caption generation \cite{xu2015show} limit the number of attention hops to a single one. A later extension is the structured self-attention \cite{lin2017structured} which is an augmented version of the standard attention that extends the single hop attention into a multi-hop one. In this thesis, I start from the premise that a multi-step attention is crucial to do an effective cross-modal retrieval. In regards to that, the contributions of this thesis are the following:
\begin{itemize}
    \item I extend the \textit{Structured self-attention} \cite{lin2017structured} for the cross-modal retrieval problem where I conduct a series of experiments to prove that a multi-hop attention is necessary in order to find all alignments between an image and a sentence.
    \item I empirically prove that using multiple hops of attention on its own is not enough for the model to leverage the increased capacity. As a result, the attention hops diversification term also introduced in \cite{lin2017structured} is implemented for the model make use of the multi-hop attention.
    \item The two branch \textit{multi-hop attention} is merged together in one attention block trained jointly for both modalities. Namely, the \textit{siamese multi-hop attention} ties the attention weights for both the image encoder and text encoder branch. This is followed by an empirical analysis to demonstrate that the \textit{multi-hop attention} learns to jointly \textit{pay attention} on both modalities at once.
    \item The \textit{Siamese multi-hop attention} model demonstrates robustness and achieves state of the art results on the rather limited in size \textit{Flickr8k} dataset and competitive results on the \textit{Pascal1k} and the much larger \textit{Flickr30k} datasets.
\end{itemize}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
