\chapter{Related work}
\label{related_work}
In this section, I do a walk-through the different cross-modal retrieval methods that have performed their experiments on the \textit{Pascal1k}, \textit{Flickr8k}, and \textit{Flickr30k} dataset. In general, the methods can be classified into two categories, namely one-to-one matching methods and many-many matching methods. In the former, a global representation is extracted from both modalities and then matched using a similarity measure whereas in the latter, a sequence of local representation is extracted from both modalities and the global similarity between the modalities is computed as an average of the local similarities. 

\section{One-to-one matching methods}

\textbf{Semantic dependency tree recurrent neural network} \cite{socher2014grounded} leverages a dependency tree parser to extract a hidden representation from the sentence. Moreover, in \textbf{SDT-RNN}, the hidden representation from the image is extracted from a convolutional neural network pre-trained on \textit{ImageNet} \cite{krizhevsky2012imagenet}. Lastly, the objective function corresponds to the matching objective used to train \textit{SMHA}.\endgraf

\textbf{Deep visual semantic embedding} \cite{frome2013devise} to the best of my knowledge, is one of the first methods to leverage transfer learning on both the image encoder and sentence encoder branch. In \textbf{DeViSE}, each of the words in the sentence is embedded using the \textit{word2vec} \cite{mikolov2013distributed} model. Then, the sentence representation is obtained by taking the mean vector from all word vectors. On the other hand, a hidden representation of the image is obtained using a convolutional neural network pre-trained on \textit{ImageNet} \cite{krizhevsky2012imagenet}. Next, both encoded modalities are projected in a joint latent space by a fully-connected layer. The objective function minimized corresponds to the matching objective used to train \textit{SMHA}.\endgraf

\textbf{Associating Neural Word Embeddings with Deep Image Representations
using Fisher Vectors} \cite{klein2015associating} is a method that achieves \textit{SOTA} results on the \textit{Pascal1k} dataset on most of the metrics. \textbf{FV} is a method that encodes the sentence as a set of \textit{word2vec} \cite{mikolov2013distributed} vectors. The image embedding is taken from a convolutional neural network pre-trained on \textit{ImageNet}. The novelty in \cite{klein2015associating} is that the sets are converted to a Fisher vector using Gaussian Mixture Model, a Laplacian Mixture Model or Hybrid Gaussian-Laplacian Mixture Model. Finally, the scoring function is computed using the canonical correlation analysis.\endgraf

\textbf{Multimodal neural language model} \cite{kiros2014unifying} is one of the first methods proving that a simple model can achieve competitive results on the image-text cross-modal retrieval task. \textbf{MNLM} obtains the image embedding from a convolutional neural network \cite{krizhevsky2012imagenet} whereas the sentence embedding is obtained from the last hidden state of an \textit{LSTM} \cite{hochreiter1997long}. Once both modalities are encoded by a separate branch, their corresponding hidden representations are projected in a joint latent space. The objective function corresponds to the matching loss used to train \textit{SMHA}.\endgraf

\textbf{Multimodal recurrent neural networks} \cite{mao2014explain} is a recurrent neural network based language model, which given an image encoding and a sentence, outputs the probability for each of the words in the sentence. The \textbf{m-RNN} during training is conditioned on the image embedding from a \textit{VGG} \cite{simonyan2014very}, and it is trained to minimize the cross-entropy of the next word in the sequence. During inference, in case the task at hand is sentence retrieval given an image, \textbf{m-RNN} outputs a probability distribution for each of the sentences conditioned on the image embedding. On the other hand, if the task being solved is image retrieval given a sentence, \textbf{m-RNN} provides a probability distribution on the query sentence conditioned on all images in the dataset.\endgraf

\textbf{Visual semantic embeddings++} \cite{faghri2017vse++} is an improvement of \cite{kiros2014unifying}. \textbf{VSE++} takes inspiration from \cite{schroff2015facenet} and leverages training only on the hard negatives in a batch. Compared to \cite{kiros2014unifying}, \textbf{VSE++} also makes use of a better image encoder such as \textit{Resnet152} \cite{he2016deep} instead of \textit{VGG} \cite{simonyan2014very}. The novelty of \cite{faghri2017vse++} is that for each anchor and positive sample in the batch, the hardest negative sample is selected. This is motivated by the fact that the hardest negatives determine the success of the \textit{Recall@1} metric \cite{faghri2017vse++}. Consequently, by training only on the hardest negatives, \textit{VSE++} obtains significant improvement on the \textit{Recall@1} metric.\endgraf

\textbf{Multimodal Convolutional Neural Network} \cite{ma2015multimodal} is a multimodal matching model that leverages convolutional neural networks for encoding the modalities and matching them. Namely, a convolutional neural network \cite{simonyan2014very} pre-trained on \textit{ImageNet} \cite{deng2009imagenet} is used to encode the image. Then, another \textit{matching} convolutional neural network takes the sentence representation together with the image representation and produces a joint representation. Lastly, a feed-forward neural network is used to translate the joint representation in a matching score between the image and the sentence. The whole architecture is jointly trained by backpropagation \cite{rumelhart1985learning} to optimize the contrastive loss objective function.\endgraf

\textbf{Embedding network} \cite{wang2018learning}, or shortly \textbf{EmbeddingNet}, uses a bidirectional recurrent neural network to encode the sentence into a fixed-size vector and a \textit{VGG} \cite{simonyan2014very} to obtain a hidden representation of the image. Subsequently, two branches of the same structure project both of the encoded modalities in a joint latent space followed by an L2 normalization. Each of the branches has an architecture that consists of a fully-connected layer followed by a \textit{ReLu} \cite{nair2010rectified} non-linearity, which is in turn followed by another fully-connected layer to project each of the branches in the joint embedding space. The objective function minimized by backpropagation aligns with the matching loss to train \textit{SMHA}.\endgraf

\section{Many-to-many matching methods}
\textbf{Deep fragment embeddings} \cite{karpathy2014deep} is a method that makes an explicit assumption that images are complex structures that contain multiple fragments. The sentences, however, are soft descriptions about the contents of the image. Therefore, to reason about their similarity, \textbf{DFE} \cite{karpathy2014deep} breaks both the image and the sentence in several fragments. Namely, to break the image in image fragments, the top 19 detected regions are extracted using an \textit{R-CNN} \cite{girshick2014rich}. On the other hand, the sentence is broken down in sentence fragments by considering the edges of a dependency tree. Then, every sentence fragment consists of a dependency triplet $(R, w_1, w_2)$ that is mapped in the embedding space by a two-layer neural network. The objective function incorporates both a fragment alignment objective as well as a global objective.\endgraf

\textbf{Deep visual-semantic alignments model} \cite{karpathy2015deep} builds on top of \textbf{DFE} \cite{karpathy2014deep}. It alleviates the bottleneck of extracting the word embeddings using dependency tree parsers where the context window is fixed, and a bidirectional recurrent neural network is used instead. Moreover, improvements are made on the image embedding branch of the model as well. In \cite{karpathy2014deep}, the top 19 image regions are extracted with an \textit{R-CNN}, \cite{girshick2014rich}, which is also the case in \cite{karpathy2015deep}. However, in \cite{karpathy2014deep} for each of the 19 proposed image regions, an image embedding is obtained using \cite{krizhevsky2012imagenet} whereas in \textbf{DVSA} \cite{karpathy2015deep} the image region embeddings are obtained using the much deeper \textit{VGG} network \cite{simonyan2014very}. The objective function is identical to \cite{karpathy2014deep}.

\textbf{Selective multimodal \textit{LSTM}} \cite{huang2017instance} is a model architecture that leverages attention \cite{bahdanau2014neural} to find the best alignment between the image and sentence embedding. \textbf{sm-LSTM} follows a three-step process. In the first step, the sentence instance candidates are obtained using a bidirectional recurrent neural network. Moreover, the image instance candidates are obtained by evenly dividing the image into regions which are passed through a convolutional neural network. In the second step, attention is applied on top of the image and sentence segments to find the best possible alignment. In the third step, the \textbf{sm-LSTM} weights the alignments to describe both modalities, and computes their global similarity through a feed-forward neural network. The whole architecture is trained end to end by minimizing the contrastive loss.\endgraf

\textbf{Dual attention networks} \cite{nam2017dual} is a two-branch network that leverages attention \cite{bahdanau2014neural} on both the image and the sentence branch. Therefore, \textbf{DAN} is closely related to the work done in the scope of this thesis. Namely, in \textbf{DAN}, two vectors of attended values, one for the image and one for the sentence are updated \textit{K} times. After each update, a dot product similarity is computed between the values. Lastly, \textbf{DAN} sums the similarities to compute the final similarity between the image and the sentence. The objective function corresponds to \cite{karpathy2014deep, karpathy2015deep, kiros2014unifying} as well as the matching loss in this thesis.\endgraf

\textbf{Stacked Cross Attention for Image-Text Matching} \cite{lee2018stacked} is an attention-based method for cross-modal retrieval that currently achieves \textit{SOTA} results on the \textit{Flickr30k} dataset. On the image encoder branch, \textbf{SCAN} extracts the top 19 image regions from an \textit{R-CNN}. Each of the image regions is encoded using a convolutional neural network \cite{he2016deep} pre-trained on \textit{ImageNet}. The word representations are obtained by taking the average of the forward and backward hidden states of a bidirectional recurrent neural network. Next, attention is applied to attend on the words of the sentence for each image region or vice versa. Once the image-text or text-image attention is applied, the model uses the cosine similarity as a similarity measure combined with the \textit{max-of-hinges} triplet loss \cite{faghri2017vse++}. In particular, \cite{lee2018stacked} report that an improvement of \textit{48.2\%} on the \textit{Recall@1} is achieved by training only on the hardest negatives within a batch \cite{faghri2017vse++}.