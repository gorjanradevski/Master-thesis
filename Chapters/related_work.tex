\chapter{Related work}
\label{related_work}
In this section, firstly I describe the fundamental building block of \textit{SMHA}, namely structured self-attention initially introduced by \citet{lin2017structured} for sentiment analysis and entailment. In addition, I lay out the main differences between the work done in this thesis and the work of \citet{lin2017structured}. Then, I do a walk-through the different image-text matching methods that have performed their experiments on the \textit{Pascal1k}, \textit{Flickr8k}, and \textit{Flickr30k} datasets. In general, the methods can be classified into two categories, namely one-to-one matching methods and many-to-many matching methods.


\section{Structured self-attention}
Self-attention emerged as a way to overcome the burden of having the recurrent neural network carry the semantics of the whole sequence for indefinite time steps, in order to represent the sequence \cite{cheng2016long, parikh2016decomposable, yang2016hierarchical, lin2017structured}. This is done by computing a single linear combination of the recurrent neural network's hidden states. However, \citet{lin2017structured} hypothesize that computing a single linear combination of the hidden states is not always sufficient to encode all bits of information. In particular, this is stressed when the sequences are fine-grained and include a lot of relations between the entities, which is the case of encoding long sequences. Therefore, \citet{lin2017structured} suggest using structured self-attention, where multiple linear combinations of the hidden states are computed instead. In this thesis, I argue that performing a single hop of attention is not sufficient to solve the image-text matching problem, and propose to use an augmented variant of structured self-attention called multi-hop attention. Even though the structured self-attention of \citet{lin2017structured} is concurrent to the multi-hop attention proposed in this thesis, the following discrepancies between the two are:
\begin{itemize}
    \item \citet{lin2017structured} used their model for sentiment analysis and entailment, while \textit{SMHA} is used for image-text matching.
    \item Structured self-attention is used to obtain sentence embeddings, followed by a task specific feed-forward neural network with softmax. On the other hand, \textit{SMHA} utilizes multi-hop attention where the attention hops are applied directly on the visual-semantic embeddings to emphasize the salient image regions and sentence words.
\end{itemize}

\section{One-to-one matching methods}
In the one-to-one matching methods, a global representation is extracted from both modalities. In particular, the current default approach is to embed the image using a deep convolutional neural network \cite{simonyan2014very, he2016deep} and encode the sentence using an \textit{LSTM} \cite{hochreiter1997long} or \textit{GRU} \cite{cho2014learning}. Then, both the image embedding and the sentence embeddings are matched using a similarity measure.\endgraf

\textbf{Semantic dependency tree recurrent neural network} \cite{socher2014grounded} leverages a dependency tree parser to extract a hidden representation from the sentence. Moreover, \textbf{SDT-RNN} extracts the image hidden representation from a convolutional neural network pre-trained on \textit{ImageNet} \cite{krizhevsky2012imagenet}. Lastly, the objective function corresponds to the matching objective used to train \textit{SMHA}.\endgraf

\textbf{Deep visual semantic embedding} \cite{frome2013devise}, to the best of my knowledge, is one of the first methods to leverage transfer learning on both the image encoder and sentence encoder branch. In \textbf{DeViSE}, each of the words in the sentence is embedded using the \textit{word2vec} \cite{mikolov2013distributed} model. Then, the sentence representation is obtained by taking the mean vector from all word vectors. On the other hand, a hidden representation of the image is obtained using a convolutional neural network pre-trained on \textit{ImageNet} \cite{krizhevsky2012imagenet}. Next, both encoded modalities are projected in a joint latent space by a fully-connected layer. The objective function minimized corresponds to the matching objective used to train \textit{SMHA}.\endgraf

\textbf{Associating Neural Word Embeddings with Deep Image Representations
using Fisher Vectors} \cite{klein2015associating} is a method that achieves \textit{SOTA} results on the \textit{Pascal1k} dataset on most of the \textit{Recall@K} metrics. \textbf{FV} is a method that encodes the sentence as a set of \textit{word2vec} \cite{mikolov2013distributed} vectors. The image embedding is taken from a convolutional neural network pre-trained on \textit{ImageNet}. The novelty of \citet{klein2015associating} is that the sets are converted to a Fisher vector using a Gaussian Mixture Model, a Laplacian Mixture Model, or Hybrid Gaussian-Laplacian Mixture Model. Finally, the scoring function is computed using the canonical correlation analysis.\endgraf

\textbf{Multimodal neural language model} \cite{kiros2014unifying} is one of the first methods proving that a simple model can achieve competitive results on the image-text matching task. \textbf{MNLM} obtains the image embedding from a convolutional neural network \cite{krizhevsky2012imagenet} whereas the sentence embedding is obtained from the last hidden state of an \textit{LSTM} \cite{hochreiter1997long}. Once both modalities are encoded by a separate branch, their corresponding hidden representations are projected in a joint latent space. The objective function corresponds to the matching loss used to train \textit{SMHA}.\endgraf

\textbf{Multimodal recurrent neural networks} \cite{mao2014explain} is a recurrent neural network based language model, which given an image encoding and a sentence, outputs the probability for each of the words in the sentence. The \textbf{m-RNN} during training is conditioned on the image embedding from a \textit{VGG} \cite{simonyan2014very}, and it is trained to minimize the cross-entropy of the next word in the sequence. During inference, in case the task at hand is sentence retrieval given an image, \textbf{m-RNN} outputs a probability distribution for each of the sentences conditioned on the image embedding. On the other hand, if the task being solved is image retrieval given a sentence, \textbf{m-RNN} provides a probability distribution on the query sentence conditioned on all images in the dataset.\endgraf

\textbf{Visual semantic embeddings++} \cite{faghri2017vse++} is an improvement of \citet{kiros2014unifying}. \textbf{VSE++} takes inspiration from \citet{schroff2015facenet} and leverages training only on the hard negatives in a batch. Compared to the approach of \citet{kiros2014unifying}, \textbf{VSE++} makes use of a better image encoder such as \textit{Resnet152} \cite{he2016deep} instead of \textit{VGG} \cite{simonyan2014very}. The novelty of \citet{faghri2017vse++} is that for each anchor and positive sample in the batch, the hardest negative sample is selected. This is motivated by the fact that the hardest negatives determine the success of the \textit{Recall@1} metric \cite{faghri2017vse++}. Consequently, by training only on the hardest negatives, \textbf{VSE++} obtains significant improvement on the \textit{Recall@1} metric.\endgraf

\textbf{Multimodal Convolutional Neural Network} \cite{ma2015multimodal} is a multimodal matching model that leverages convolutional neural networks for encoding the modalities and matching them. Namely, a convolutional neural network \cite{simonyan2014very} pre-trained on \textit{ImageNet} \cite{deng2009imagenet} is used to encode the image. Then, another \textit{matching} convolutional neural network takes the sentence representation together with the image representation and produces a joint representation. Lastly, a feed-forward neural network is used to translate the joint representation in a matching score between the image and the sentence. The whole architecture is jointly trained by backpropagation \cite{rumelhart1985learning} to optimize the contrastive loss objective function.\endgraf

\textbf{Embedding network} \cite{wang2018learning}, or shortly \textbf{EmbeddingNet}, uses a bidirectional recurrent neural network to encode the sentence into a fixed-size vector and a \textit{VGG} \cite{simonyan2014very} to obtain a hidden representation of the image. Subsequently, two branches of the same structure project both of the encoded modalities in a joint latent space followed by an L2 normalization. Each of the branches has an architecture that consists of a fully-connected layer followed by a \textit{ReLu} non-linearity \cite{nair2010rectified}, which is in turn followed by another fully-connected layer to project each of the branches in the joint embedding space. The objective function minimized by backpropagation aligns with the matching loss to train \textit{SMHA}.\endgraf

\section{Many-to-many matching methods}

The many-to-many matching methods are characterized by obtaining many local representations from the image and the sentence. Namely, the sequence of local image representations is obtained by dividing the image into regions. Furthermore, the local sentence representations are the \textit{LSTM} \cite{hochreiter1997long} or \textit{GRU} \cite{cho2014learning} hidden states for each of the words. Then, the sequence of local image and sentence representations are matched so that the global matching score can be computed as an average of the local matching scores. 

\textbf{Deep fragment embeddings} \cite{karpathy2014deep} is a method which makes an explicit assumption that images are complex structures which contain multiple fragments. The sentences, however, are soft descriptions about the contents of the image. Therefore, to reason about their similarity, \textbf{DFE} \cite{karpathy2014deep} breaks both the image and the sentence in several fragments. Namely, to break the image in image fragments, the top 19 detected regions are extracted using an \textit{R-CNN} \cite{girshick2014rich}. On the other hand, the sentence is broken down in sentence fragments by considering the edges of a dependency tree. Then, every sentence fragment consists of a dependency triplet that is mapped in the embedding space by a two-layer neural network. The objective function incorporates both a fragment alignment objective as well as a global objective.\endgraf

\textbf{Deep visual-semantic alignments model} \cite{karpathy2015deep} builds on top of \citet{karpathy2014deep}. It alleviates the bottleneck of extracting the word embeddings using dependency tree parsers where the context window is fixed, and instead, it uses a bidirectional recurrent neural network. Moreover, improvements are made on the image embedding branch of the model as well. In \citet{karpathy2014deep}, the top 19 image regions are extracted with an \textit{R-CNN} \cite{girshick2014rich}, which is also the case in \textbf{DVSA} \cite{karpathy2015deep}. However, in \citet{karpathy2014deep}, for each of the 19 proposed image regions, an image embedding is obtained with \textit{AlexNet} \cite{krizhevsky2012imagenet} whereas in \textbf{DVSA} \cite{karpathy2015deep} the image region embeddings are obtained using the much deeper \textit{VGG} network \cite{simonyan2014very}. The objective function is identical to the one used in \citet{karpathy2014deep}.

\textbf{Selective multimodal \textit{LSTM}} \cite{huang2017instance} is a model architecture that leverages attention \cite{bahdanau2014neural} to find the best alignment between the image and sentence embedding. \textbf{sm-LSTM} follows a three-step process. In the first step, the sentence instance candidates are obtained using a bidirectional recurrent neural network. Moreover, the image instance candidates are obtained by evenly dividing the image into regions which are embedded using a convolutional neural network. In the second step, attention is applied on top of the image and sentence segments to find the best possible alignment. In the third step, the \textbf{sm-LSTM} weights the alignments to describe both modalities, and computes their global similarity through a feed-forward neural network. The whole architecture is trained end to end by minimizing the contrastive loss.\endgraf

\textbf{Dual attention networks} \cite{nam2017dual} is a two-branch model that leverages attention \cite{bahdanau2014neural} on both the image and the sentence branch. Therefore, \textbf{DAN} \cite{nam2017dual} is closely related to the work done in the scope of this thesis. Namely, in \textbf{DAN}, two vectors of attended values, one for the image and one for the sentence are updated \textit{K} times. After each update, a dot product similarity is computed between the values. Lastly, \textbf{DAN} sums the similarities to compute the final similarity between the image and the sentence. The objective function corresponds to other approaches \cite{karpathy2014deep, karpathy2015deep, kiros2014unifying} as well as the matching loss in this thesis.\endgraf

\textbf{Stacked Cross Attention for Image-Text Matching} \cite{lee2018stacked} is an attention-based method for image-text matching that currently achieves \textit{SOTA} results on the \textit{Flickr30k} dataset. On the image encoder branch, \textbf{SCAN} extracts the top 19 image regions from an \textit{R-CNN}. Each of the image regions is encoded using a convolutional neural network \cite{he2016deep} pre-trained on \textit{ImageNet}. The word representations are obtained by taking the average of the forward and backward hidden states of a bidirectional recurrent neural network. Next, attention is applied to attend to the words of the sentence for each image region or vice versa. Once the image-text or text-image attention is applied, the model uses the cosine similarity as a similarity measure combined with the \textit{max-of-hinges} triplet loss \cite{faghri2017vse++}. In particular, \citet{lee2018stacked} report that an improvement of \textit{48.2\%} on the \textit{Recall@1} is achieved by training only on the hardest negatives within a batch \cite{faghri2017vse++}.