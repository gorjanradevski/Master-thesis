\chapter{Related work}
\label{related_work}
In this section I will do a walk-through the different cross-modal retrieval methods that have performed their experiments on the \textit{Pascal1k}, \textit{Flickr8k} and \textit{Flickr30k} datasets. In general, the methods can be classified into two categories, namely one-to-one matching methods and many-many matching methods. In the former, a global representation is extracted from both modalities and then matched using a similarity measure whereas in the latter, a sequence of local representation is extracted from both modalities and the global similarity between is computed as an average of the local similarities. 
\section{One-to-one matching methods}

\textbf{Semantic dependency tree recurrent neural network} \cite{socher2014grounded} leverages a dependency tree parser in order to extract a hidden representation from the sentence. Moreover, in \textbf{SDT-RNN} the hidden representation from the image is extracted from a convolutionla neural network pretrained on \textit{ImageNet} \cite{krizhevsky2012imagenet}. Lastly, the objective function matches the global objective function from \cite{karpathy2014deep} as well as the mismatching part of the objective function used in this thesis.\endgraf

\textbf{Deep visual semantic embedding} \cite{frome2013devise} to the best of my knowledge, is one of the first methods to leverage transfer learning on both the image encoder and text encoder branches. In \textbf{DeViSE}, each of the words of the sentence are encoded as one-hot vectors which are later embedded using the \textit{Skip-gram} \cite{mikolov2013distributed} model. Then, the sentence representation is obtained by taking the mean vector from all of the word vectors. On the other hand, a hidden representation of the image is obtained using a convolutional neural network pretrained on \textit{ImageNet} \cite{krizhevsky2012imagenet}. Because both of the image embedding and the sentence embedding are in their corresponding latent spaces, each of the modality embeddings are projected into a joint latent space by a one-layer neural network which is followd by the objective function to be minimized. The mismatching part of the objective function used within this thesis matches the one used in \cite{frome2013devise}.\endgraf

\textbf{Associating Neural Word Embeddings with Deep Image Representations
using Fisher Vectors} \cite{klein2015associating} is a method that achieves \textit{SOTA} results on the Pascal1k on most of the metrics. \textbf{FV} is a method that encodes the sentence as a set of word2vec \cite{mikolov2013distributed} vectors. The image embedding on the other hand, is taken from a convolutional neural network pretrained on \textit{ImageNet}. The novelty in \cite{klein2015associating} is that the sets are converted to a Fisher vector using Gaussian Mixture Model, a Laplacian Mixture Model or Hybrid Gaussian-Laplacian Mixture Model. Finally, the scoring function computed using the cannonical correlation analysis.\endgraf

\textbf{Multimodal neural language model} introduced by \cite{kiros2014unifying}, is one of the methods that indicates that a simple model may actually perform really well on the cross-modal retrieval task. The \textbf{MNLM} extracts image embedding from \cite{krizhevsky2012imagenet} together with the sentence embedding which are obtained from the last-hidden state of an \textit{LSTM}\cite{hochreiter1997long}. Once both modalities are encoded by a separate branch, the encodings are projected into a joint latent space. The objective function optimized matches the global alignment objective from \cite{karpathy2014deep, karpathy2015deep} as well as the matching loss of this thesis.\endgraf

\textbf{Multimodal recurrent neural networks}\cite{mao2014explain} is a recurrent neural network based language model that given an image encoding and a sentence, it outputs a probability of the occurrence for each of the words in the sentence. The \textbf{m-RNN} during training is conditioned on the image embedding from a VGG16/19 \cite{simonyan2014very}, and it is trained to minimize the cross-entropy of the next ground truth word in a sequence. During inference, in case the task at hand is sentence retrieval given an image, the \textbf{m-RNN}, will output a probability distribution for each of the sentences in the dataset conditioned on the image embedding. On the other hand, if the task being solved is image retrieval given a sentence, the \textbf{m-RNN} will provide a probability distribution on the query sentence, conditioned on all of the images in the dataset. Therefore, in both cases the output can be seen considered as a ranked list.\endgraf

\textbf{Visual semantic embeddings++} \cite{faghri2017vse++} is an improvement of \cite{kiros2014unifying}. The \textbf{VSE++} takes inspiration from \cite{schroff2015facenet}, and leverages training the model on the hard negatives instead of on the semi-hard and hard negatives. Compared to \cite{kiros2014unifying}, in \textbf{VSE++} they leverage the use of a better image encoder such as \textit{Resnet152} \cite{he2016deep}, instead of \textit{VGG} \cite{simonyan2014very}. The novelty of \cite{faghri2017vse++} is that for each anchor image and a positive sentence in the batch, the hardest negative contrastive sentence selected and vice versa. This is motivated by the fact that the hardest negatives determine the success for the \textit{Recall@1} metric. Consequently, by training only on the hardest negatives, \cite{faghri2017vse++} obtain significant improvement especially on the \textit{Recall@1} metric.\endgraf

\textbf{Multimodal Convolutional Neural Network} \cite{ma2015multimodal} is a multi-modal matching model that leverages convolutional neural networks only for both encoding the modalities and matching them. Namely, a convolutional neural network \cite{simonyan2014very} pretrained on \textit{ImageNet} \cite{deng2009imagenet} is used to encode the image. Then, another \textit{matching} convolutional neural network, takes the word/sentence representation together with the image representation and produces a joint representation. Lastly, a feed-forward neural network is used to translate the joint representation into a matching score between the image and the sentence. The whole architecture is jointly trained by backpropagation \cite{rumelhart1985learning} to optimize the contrastive loss objective function.\endgraf

\textbf{Embedding network} \cite{wang2018learning}, or shortly \textbf{EmbeddingNet} leverages a bidirectional recurrent neural network to encode the sentence into a fixed size vector, and a \textit{VGG} \cite{simonyan2014very}, to obtain a hidden representation of the image. Afterwards, two branches that share the same structure but have separate weights project both of the encoded modalities in a joint hidden space followed by a L2 normalization. Each of the branches has an architecture that consists of a fully connected layer followed by a ReLu \cite{nair2010rectified} non linearity, which is in turn followed by a projection layer in the joint embedding space. The objective function minimized by backpropagation aligns with the matching loss used within the scope of this thesis.\endgraf

\section{Many-to-many matching methods}
\textbf{Deep fragment embeddings} \cite{karpathy2014deep} is a method that makes an explicit assumption that images are complex structures which have multiple entities in them. The sentences however, are soft descriptions about the contents of the image. Therefore, in order to reason about their similarity, \textbf{DFE} \cite{karpathy2014deep} breaks both the image and the sentence into several fragments. Namely, to break the image into image fragments the top 19 detected regions are extracted using an R-CNN \cite{girshick2014rich}. On the other hand, the sentence is broken down into sentence fragments by considering the edges of a dependency tree. Then, every sentence fragment constitutes of a dependency triplet $(R, w_1, w_2)$, where the dependency triplet is mapped into the embedding space by a two-layer neural network. Th3 objective function incorporates both a fragment alignment objective as well as a global objective, where the global objective function matches the mismatching part of the objective function used in this thesis.\endgraf

\textbf{Deep visual-semantic alignments model} \cite{karpathy2015deep}, builds on top of \textbf{DFE} \cite{karpathy2014deep}. It alleviates the bottleneck of extracting the word embeddings using dependency tree parsers, where the context window is fixed, and they use a bidirectional recurrent neural network instead. Moreover, improvements are done on the image embedding branch of the model as well. Namely, in \cite{karpathy2014deep} the top 19 image regions are extracted with an R-CNN \cite{girshick2014rich} which is also the case in \cite{karpathy2015deep}. However, in \cite{karpathy2014deep} for each of these 19 proposed image regions an image embedding is obtained using \cite{krizhevsky2012imagenet} whereas in \textbf{DVSA} \cite{karpathy2015deep} the image region embeddings are obtained using the much deeper VGG network \cite{simonyan2014very}. Afterwards, following \textbf{DFE}, both a fragment and a global alignment are computes as an objective function.\endgraf

\textbf{Selective multimodal LSTM} \cite{huang2017instance} is a model architecture that leverages attention \cite{bahdanau2014neural} in order to find the best alignment between the image embedding and the sentence embedding. \textbf{sm-LSTM}, follows a three step process. In the first step, the instance candidates are obtained using a bidirectional recurrent neural network for the sentence and by evenly dividing the image into regions and passing them through a \textit{CNN}. In the second step, attention is applied on top of the segments from the sentence and the image in order to extract the best possible alignment between them. Once the alignments are found, in the third step the \textbf{sm-LSTM} weights the alignments to describe the image and the sentence and computes the global similarity through a feed-forward neural network. The whole architecture is trained end to end by minimizing the contrastive loss.\endgraf


\textbf{Dual attention networks} \cite{nam2017dual} is a two branch network that leverages attention \cite{bahdanau2014neural} on both the image and the sentence branch. Therefore, the \textbf{DAN} architecture is closely related to the work done within the scope of this thesis. However, the attention mechanism used in \textbf{DAN} and within this thesis are fundamentally different. Namely, in \textbf{DAN}, two vectors of attended values, one for the image and one for the text are updated \textit{K} times. After each update, a dot product similarity is computed between the values. Lastly, \textbf{DAN} sums all the similarities in order to compute the final similarity between the image and the text. The minimized loss function corresponds to \cite{karpathy2014deep, karpathy2015deep, kiros2014unifying} as well as the matching loss used within the scope of this thesis.\endgraf

\textbf{Stacked Cross Attention for Image-Text Matching} \cite{lee2018stacked} is an attention based method for cross-modal retrieval that currently achieves \textit{SOTA} results on the \textit{Flickr30k} dataset. On the image encoder branch, \textit{SCAN} extract the top 19 image regions from a R-CNN. Each of the image regions is then encoded using a convolutional neural network \cite{he2016deep} pretrained on \textit{ImageNet}. The word representations are obtained by taking the average of the forward and backward hidden states of a Bi-RNN which is trained from scratch. After that, attention is applied so that the model can attend on the words of the sentence for each image region or vice versa. Once the image-text or text-image attention is applied, the model uses the cosine similarity objective function combined with the \textit{max-of-hinges} triplet loss \cite{faghri2017vse++}. In particular, in \cite{lee2018stacked} it is reported that an improvement of \textit{48.2\%} on the \textit{Recall@1} is achieved by training only on the hardest negatives within a batch \cite{faghri2017vse++}.