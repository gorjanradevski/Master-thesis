\chapter{The model}
\label{The model}
In this section, I go through the \textit{Siamese multi-hop attention} model in a bottom-up fashion. I start at the lower end of the model, where the raw images and the corresponding sentences enter the model and are met by the data preprocessing and augmentation modules. At the second level of the model architecture are both modality encoders, namely the image encoder which is deep convolutional neural network \cite{he2016deep} pre-trained on \textit{ImageNet} \cite{deng2009imagenet}, and the sentence encoder which is a language model \cite{peters2018deep} pre-trained on the 1 Billion word benchmark dataset \cite{chelba2013one} in conjunction with a bidirectional \textit{GRU} \cite{cho2014learning}. Going upward is the attention module where both outputs from the modality encoders are fed in so that the model can attend on the salient segments of the image and the sentence. The second to last part is the loss computation module where several different regularization methods are implemented in addition to the standard matching loss in order to compute the final loss. Lastly, the loss is fed in the optimizer module where the gradients are computed and back-propagated through the network \cite{rumelhart1985learning} to update the network weights. The model architecture is illustrated on figure \ref{fig:full_model}.

\begin{figure}
  \centering
  \includegraphics[width=70mm]{Images/full_model.png}
  \caption[The full model architecture]{The full model architecture consists of the modality encoder branches, namely the image encoder and the sentence encoder. Next is the \textit{siamese multi-hop attention} module where the model attends on the salient image regions and sentence words. The loss module computes the matching loss which added together with the regularization loss and passed to the optimizer.}
  \label{fig:full_model}
\end{figure}

\section{Data processing}

All of the datasets (with no particular order \cite{rashtchian2010collecting, hodosh2013framing, young2014image}) used to conduct the research in this thesis contain pairs of images and sentences that match one another. Even though the datasets are rather small as per the amount of data that the deep neural networks can consume, processing them all at once is not feasible. Because of that, a sequential process is implemented where batches of image-sentence pairs are read from the disk, preprocessed, and passed further up the model, so that the training and inference processes can run efficiently. This section sheds light on the processing of the data batches before they are fed into the model.

\subsection{Image preprocessing}
The first step of the image preprocessing pipeline is to prepare the data in the format that the model requires. In \textit{SMHA}, the choice of image preprocessing is tightly coupled to the preprocessing the image encoder used when it was trained. Because the image encoder in \textit{SMHA} is a \textit{ResNet152} \cite{he2016deep} pre-trained on \textit{ImageNet} \cite{deng2009imagenet}, the inputs are preprocessed according to the preprocessing regime employed when the \textit{ResNet152} was trained \cite{simonyan2014very}.\endgraf
The next segment of the image processing pipeline is image augmentation. Data augmentation is a series of techniques employed to randomly distort the training data, mostly used when the amount of data at hand is limited. By including such noise in the data, the expected outcome is the model repeatedly treating the malformed data samples as new, and thus reducing the risk of the model overfitting on the training data. In the standard supervised learning setting $X \rightarrow Y$, the data augmentation techniques are applied on \textit{X}, where each training batch of data is distorted using a series of specific transformations with a random magnitude. The series of transformations applied during training the image encoder of \textit{SMHA} illustrated in figure \ref{fig:image_augmentation} are:
\begin{itemize}
  \item Random 224x224 centered crop of the image
  \item Random horizontal flip with 50\% probability.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=100mm]{Images/data_augmentation.png}
  \caption[Image augmentation]{The image augmentation module follows a two-step process  to distort the image randomly. Firstly, a random centred crop is extracted from the image. Secondly, the image is flipped horizontally with 50\% probability.}
  \label{fig:image_augmentation}
\end{figure}

On the other hand, during inference, the transformations are switched off, and the images are re-sized to have 224 pixels width and height.

\subsection{Text preprocessing}
For the sentence encoder to extract meaningful representations for each of the words in the sentence, the sentences have to be cleaned and tokenized. In the cleaning phase, before tokenizing the sentence, the following transformations are applied to the sentences:
\begin{itemize}
  \item The words in the sentences are lowercased
  \item All punctuation is removed from the sentences.
\end{itemize}
By doing this, inevitably, words which are necessarily the same would not be treated as separate words in the vocabulary space due to having an uppercase starting letter or ending with a particular punctuation mark.

\section{Modality encoders}
In this section, I do a walkthrough of both of the modality encoders, namely the image encoder and the sentence encoder. Section \ref{sec: image_encoder} provides a detailed overview of the image encoder branch of the model, whereas a section \ref{sec: sentence_encoder} provides a detailed overview of the sentence encoder.

\subsection{Image encoder}\label{sec: image_encoder}
The image encoder is a deep convolutional neural network \textit{ResNet152} \cite{he2016deep} pre-trained on \textit{ImageNet} \cite{deng2009imagenet}, in conjunction with a fully-connected layer trained from scratch on top of the \textit{ResNet152} to project the image embedding in the joint latent space. The family of \textit{ResNets} emerged as a way to overcome the degradation problem that occurs when training increasingly deeper neural networks \cite{he2015convolutional}. The degradation problem is a phenomenon that occurs when stacking multiple layers on a neural network. Namely, the expected behavior is proportionally increasing training accuracy when the number of hidden layers in a neural network increases. However, in the degradation problem, the increasing depth of the neural network comes with the repercussion of the training accuracy saturating and then decreasing rapidly. Since the accuracy is computed on the training set, this phenomenon is unrelated to overfitting, which is the case where the neural network's performance increases on the training set but decreases on the validation set. The \textit{degradation} phenomenon has been investigated and empirically proven by \citet{he2016deep, he2015convolutional}. Therefore, to overcome the degradation problem, \citet{he2015convolutional} introduced the residual block. The main purpose of the residual block is to sidestep learning an explicit mapping between a block of stacked layers, and let the stacked layers learn a residual mapping instead. To do so, the original mapping is recast into:

$$F(x) + x$$

by letting the the stacked layers fit the mapping:

$$F(x) = H(x) - x$$

where $H(x)$ is the desired mapping that we originally wanted to learn and \textit{x} is the input to the block of stacked layers. Namely, if a neural network with $N$ layers can sufficiently approximate the desired function $F(x)$, then a neural network with $N + M$ layers where the newly added stack of layers are constructed as identity mappings, should approximate the same function just as good. Based on this hypothesis, it follows that the newly added stack of layers has difficulties approximating the identity function. By having the residual connection, if the identity mapping between the layers is desired, the neural network can successfully learn to push its weights towards zero and thus fit the identity mapping. Therefore, the building block of the \textit{ResNet} family of neural networks is:

$$y = W_2 \sigma(W_1x) + x$$

Where $W_1$ and $W_2$ are the weight matrices to be learned, $x$ is the input and $\sigma$ denotes the \textit{ReLU} \cite{nair2010rectified} activation function.\endgraf

In the scope of this thesis, a \textit{ResNet152} pre-trained on \textit{ImageNet} is used, while a feature vector from the image is extracted from the last convolutional layer before the logits. That results in a $7\times7\times2048$ dimensional representation of the encoded image. Namely, the activation map of the last convolutional layer of the \textit{ResNet152} results in an output of 2048 filters with a width and height of 7.
The motivation for doing such transfer learning \cite{yosinski2014transferable} is related to the empirical proof that when a convolutional neural network is trained on a dataset of images, the first few layers of the neural network learn features which are general and invariant \cite{yosinski2014transferable} of the minimized objective function. That makes the learned features easily transferable from one image domain to another. On the other hand, the layers that reside at the end of the neural network tend to specialize on the training objective. For that reason, the typical transfer learning scenario is to obtain the first few layers of a neural network trained on a bigger dataset and to append these first layers on a new neural network, that is trained on a specific task. The new, task-specific neural network includes other layers, which are randomly initialized and trained to make use of the extracted features and map them to its corresponding task. In this particular case, the new neural network is exactly one fully-connected layer that projects the \textit{ResNet152} features in the joint latent space.\endgraf

Figure \ref{fig:image_encoder} illustrates the image encoder branch of the model. On the lower end, the augmented image is provided as input, and the \textit{ResNet152} extracts the features of the image $(S_1, S_2, S_3,...,S_n)$. Each $S_i$ is of size \textit{2048} and holds a single activation element from all \textit{2048} filters. \textit{n} corresponds to the flattened activation map with size \textit{49}. After projecting the filters in the joint space with a fully-connected layer, the final output from the image encoder is a list of hidden states $(H_1, H_2, H_3,...,H_n)$ where \textit{n} remains 49, and each $H_i$ has a size that corresponds to the joint space.

\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/image_encoder.png}
  \caption[The image encoder branch]{The image encoder extracts the image features from a \textit{ResNet152} \cite{he2016deep}, pre-trained on \textit{ImageNet} \cite{deng2009imagenet}. A fully-connected layer then projects the image features in the joint latent space.}
  \label{fig:image_encoder}
\end{figure}


% SENTENCE ENCODER%


\subsection{Sentence encoder}\label{sec: sentence_encoder}
The input to the sentence encoder is a sequence of $N$ word tokens $(w_1, w_2,...,w_N)$. The commonly adopted way to encode the word tokens in hidden representations is to initially encode each word $w_t$ as a one-hot vector $[0,0,...,1,0,0]$ where the index of the vector element that is $1$, corresponds to the word index $t$. Then, each of the word vectors are embedded into $M$ dimensional vectors through a word embedding matrix $W_e^Tw_t$ \cite{nam2017dual, kiros2014unifying, wang2018learning, lee2018stacked, faghri2017vse++}. In this thesis, I argue that such an approach is undesired due to the lack of training data in the commonly used benchmark datasets. For the word embedding matrix to learn sufficiently good vector representations of words, it needs to be trained on much bigger datasets \cite{mikolov2013distributed, pennington2014glove, peters2018deep}. Moreover, I argue that initializing the word embedding matrix with a fixed pre-trained word embeddings and then encoding the sentence as a linear combination of the word vectors \cite{klein2015associating} is undesired. This is because the word meaning is not only related to the sentence context, but also to the corresponding image. In that sense, having fixed word embeddings limits the ability of the model to adjust the meaning of the word based on both the word context and the matching image. Therefore, instead of training the word embedding matrix from scratch, or using a fixed word embedding matrix, I make use of word embeddings from a bidirectional language model, or also known as \textit{ELMo} \cite{peters2018deep}.\endgraf

\textit{ELMo} are word embeddings obtained from the internal states of a bidirectional language model. Given a sequence of $N$ word tokens $(w_1, w_2,...,w_N)$, a forward language model computes the probability of the sequence by computing the probability of a word token $w_t$, given its history $(w_1, w_2,...,w_{t-1})$:
$$p(w_1, w_2,...,w_N) = \prod_{t=1}^N p(w_t \vert w_1,w_2,...,w_{t-1})$$
On the other hand, a backward language model shares the same characteristics as a forward language model, except that it computes everything in reverse. In other words, it computes the probability of the sequence by computing the probability of a word token $w_t$, given its future $(w_{t+1},w_{t+2},...,w_{N})$:
$$p(w_1, w_2,...,w_N) = \prod_{t=1}^N p(w_t \vert w_{t+1},w_{t+2},...,w_N)$$
\endgraf
The bidirectional language model used to compute \textit{ELMo} is based on the recent state-of-the-art language models \cite{melis2017state, jozefowicz2016exploring}, where initially a context-independent representation for each of the tokens is obtained using character convolutions. The obtained representation is then passed through \textit{L} layers of forward and backward \textit{LSTMs} \cite{hochreiter1997long}. In the model later used to compute \textit{ELMo}, during training, at time $t$, the \textit{LSTM} is presented with a context-independent representation of the word token $w_t$. To convert the context-independent representation into a context-dependent one, the \textit{LSTM} goes through a multi-step process that is managed by its gates. The \textit{LSTM} has 3 of these gates. The first of the \textit{LSTM} gates decides how much of the past information is going to be forgotten:

$$f_t = \sigma(W_f \cdot [h_{t-1}, w_t] + b_f)$$
Because of the role that this gate plays, it is called the forget layer gate. The second step is to decide how much of the new information $w_t$ is going to be stored in the cell:

$$i_t = \sigma(W_i \cdot [h_{t-1}, w_t] + b_i)$$
$$\hat{C_t} = tanh(W_c \cdot [h_{t-1}, w_t] + b_c)$$
$$C_t = f_t * C_{t-1} + i_t * \hat{C_t}$$

This gate updates the memory of the cell based on the present information. For that reason, it called the input layer gate. Finally, the output layer gate is responsible for providing the context-dependent representation for the word token $w_t$:

$$o_t = \sigma(W_o \cdot [h_{t-1}, w_t] + b_o)$$
$$h_t = o_t * tanh(C_t)$$

Here, 2 layers $(L = 2)$ of forward and backward \textit{LSTMs} are stacked on top of each other, followed by a softmax that returns return the probability for the next word in the sequence.\endgraf
In this thesis, to come up with \textit{ELMo} for each of the word tokens, the softmax layer is stripped off, and a weighted average of the internal states of the pre-trained bidirectional language model \cite{peters2018deep} is computed. Assuming that we are given a sequence of $N$ tokens, the \textit{ELMo} of the sequence of tokens would be a matrix $E = (e_1^{elmo}, e_2^{elmo},...,e_N^{elmo})$ where each $e_t^{elmo}$ is a 1024 dimensional vector. However, the sequence representations $E$ still resides in the \textit{bidirectional language model} latent space, and the end goal is to have the sentence embedding projected in the joint latent space together with the image encoding. To do so, I add a bidirectional \textit{GRU} \cite{cho2014learning} on top. Besides projecting the sentence embedding in the image encoder latent space, the purpose for having such a recurrent neural network on top of \textit{ELMo} is to extract higher-level context-dependent representations for each of the words tokens. On the contrary, this is not needed in the \textit{image encoder} case, and a simple fully-connected layer is used to project the image embedding in the corresponding latent space.\endgraf
The \textit{GRU} recurrent neural network is a simplified version of the \textit{LSTM} \cite{hochreiter1997long} recurrent neural network that fuses the input and update layer gates into a single update gate. Consequently, given a sequence of \textit{ELMo} for each of the word tokens $E = (e_1^{elmo}, e_2^{elmo},...,e_{t-1}^{elmo})$, the hidden state of the forward $\overrightarrow{GRU}$ recurrent neural network is computed as the result of the equations:


$$z_t = \sigma(W_z \cdot [h_{t-1}, e_t])$$
$$r_t = \sigma(W_r \cdot [h_{t-1}, e_t])$$
$$\hat{h_t} = tanh(W \cdot [r_t * h_{t-1}, e_t])$$
$$h_t = (1 - z_t) * h_{t-1} + z_t * \hat{h_t}$$

On the other hand, the backward $\overleftarrow{GRU}$ obeys to the same equations, with the only difference being that instead of considering the past hidden states, the hidden state $h_t$ is conditioned on the future hidden states $\overleftarrow{H} = (h_N, h_{N-1},...,h_{t+1})$. Finally, to get the final sentence embedding, the forward and backward hidden states are averaged:

$$H = \frac{\overrightarrow{H} + \overleftarrow{H}}{2}$$


\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/sentence_encoder.png}
  \caption[The sentence encoder branch]{The sentence encoder firstly extracts the word embeddings as a linear combination of the internal states from the bidirectional language model. Finally, from each \textit{ELMo}, a one-level deeper context-dependent representation is obtained using a \textit{GRU} recurrent neural network \cite{cho2014learning}.}
  \label{fig:sentence_encoder}
\end{figure}

On figure \ref{fig:sentence_encoder}, the sentence encoder branch of the model is portrayed. The sequence of words $(w_1, w_2, w_3,...,w_n)$ is processed by the \textit{bidirectional language model} used to compute \textit{ELMo}. The output $(e_1, e_2, e_3,...,e_n)$ is the \textit{ELMo} for each of the words in the sequence. Next, each of the \textit{ELMo} is passed through a bidirectional \textit{GRU} recurrent neural network. Finally, each of the hidden states $(h_1, h_2, h_3,...,h_n)$ are the averaged forward and backward \textit{GRU} hidden states, where \textit{n} is the length of the sequence, and the size of each $H_i$ corresponds to the size of the joint space.

% ATTENTION %

\section{Siamese multi-hop attention}

The attention mechanism, initially introduced by \citet{bahdanau2014neural}, is inspired by the human ability to pay attention to visual parts of an image or correlate words in a sentence \cite{rensink2000dynamic, corbetta2002control}. At first applied in the sequence-to-sequence translation with recurrent neural networks \cite{sutskever2014sequence} to overcome the encoder bottleneck, it was immediately extended to visual caption generation by \citet{xu2015show} as well as document classification by \citet{yang2016hierarchical}. A recently introduced variant of the attention mechanism is the self-attention \cite{cheng2016long} and its extension the structured self-attention \cite{lin2017structured}. In this thesis, I am using a variant \citet{lin2017structured, xu2015show} self-attention mechanism in order to attend multiple times at parts of both the image and the sentence. Moreover, I have empirically found that in case the weights of the attention block are reused across the modalities, the model achieves at least as good performance as the one that uses distinct attention weights for both of the modalities. Because such reusing of weight matrices is firstly introduced by \citet{bromley1994signature} and their \textit{Siamese neural network}, I call this attention mechanism the \textit{Siamese multi-hop attention} or shortly \textit{SMHA}.\endgraf
\textit{SMHA} tries to leverage the fact that multiple entities are present within a sentence and an image, and for a model to successfully do the image-text matching it has to attend multiple times on both modalities. By doing that, the outputs are attention-weighed vectors, where each vector encodes information about a distinct part of the image or the sentence. The \textit{SMHA} is applied on top of both the image encoder and the sentence encoder, and as such extracts these vectors for both modalities.\endgraf

Suppose that the output from one of the modality encoders consists of $N$ hidden states $H = (h_1, h_2, h_3,...,h_N)$. Consequently, the size of $H$ would be $(N, D)$ where $N$ is the number of hidden states, and $D$ is the dimensionality of the joint hidden space. However, we can be certain that only \textit{D} is the same for both of the outputs of the modality encoders. On the other hand, \textit{N}, which is the number of output hidden states from the \textit{sentence encoder} and the spatial output size from \textit{image encoder}, can vary from one modality to another and be of arbitrary size. As a result, to do the matching of the image embedding and the sentence embedding, both of the modalities need to be encoded with a fixed size vector. An apparent method to overcome this issue is to perform pooling on the hidden states from each of the modalities. However, when doing such pooling, all of the hidden states are going to contribute equally in the final embedding, which is undesirable. Therefore, the attention mechanism is a way to compute the final representation as a linear combination of the hidden states. The attention mechanism gets the hidden states as inputs and outputs a set of weights for each of the hidden states. The weights are computed using a one-layer feed-forward neural network with a \textit{tanh} activation function. 

$$U = tanh(W_u H + b_u)$$
$$A_{t} = \frac{e^{U^T W_a}}{\sum_t e^{U^T W_a}}$$

In the equations above, $W_u$ is the weight matrix of the hidden layer, and $W_a$ is the weight matrix of the output layer of the attention mechanism. The size of $W_u$ and $W_a$ are chosen as the values that yield the best score on the validation set. 

\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/attention_weights.png}
  \caption[Computation of the attention weights for a single attention hop]{To come up with the attention weights for a single attention hop, firstly the hidden states are passed through a one-layer feed-forward neural network with a softmax activation function on the final layer. The output is the normalized attention weights for each hidden state.}
  \label{fig:attention_weights}
\end{figure}

Figure \ref{fig:attention_weights} illustrates the process of coming up with the attention weights. Firstly the hidden representations $H$ are fed through a one-layer feed-forward neural network to extract the hidden representation $U$. Then, this is followed by a softmax layer where the softmax operation is performed along the second dimension of the input. The output is a matrix of normalized attention weights of the hidden states for each of the attention hops.\endgraf
Once the attention weights are computed and normalized, the hidden states are weighted accordingly for each attention hop. 

$$O = A \cdot U$$

Figure \ref{fig:attention_attend} illustrates the weighting process for a single attention hop. Once the attention weights are obtained, a linear combination of the hidden states $(h_1, h_2, h_3,...,h_n)$ is computed, where each hidden state $h_i$ is weighted accordingly. The multi-hop attention computes multiple sets of weights, and thus, the whole weighing process is applied multiple times.\endgraf

\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/attention_attend.png}
  \caption[Weighting of the hidden states]{Once the attention weights from a single attention hop are obtained, a linear combination of the hidden states is computed. The output is a vector that represents the linear combination of the hidden states.}
  \label{fig:attention_attend}
\end{figure}

When the input to the multi-hop attention module is the output from the image encoder, the attention weights are computed along the spatial dimension which is the $width \times height$ of the activation map of the last convolutional layer of \textit{ResNet152}. On the other hand, if the input to the multi-hop attention is the output from the sentence encoder, the attention weights are computed across the time (number of word tokens) dimension.\endgraf
Lastly, the output of the attention block is normalized to lie on the unit hypersphere by computing:

$$O_{normalized} = O / \sqrt{max(\sum_i o_i^2,  \epsilon))}$$

Where $\epsilon$ is a small number to ensure that the square root is defined in that domain.

\section{Matching loss}
In the supervised learning paradigm, more concretely classification, usually what we have is a fixed number of categories to which each of the training set instances belongs. As a result, the mapping to be learned is from a domain of the feature space $X$ to a domain of target categories $Y$. Therefore, we can train a classifier which minimizes a loss function that indicates the mismatch between the target output $Y$ and the predicted output $P$. However, there are cases where the number of categories is too big for the neural network to learn the relation $X \rightarrow Y$ or in some cases even infinite. In those cases, the need to train a classifier that can recognize whether two instances belong to the same category or not arises.\endgraf
The triplet loss function, introduced by \citet{weinberger2009distance} is a way to train a classifier when the number of distinct categories can vary in the domain $[2, \infty]$. The goal of the triplet loss is to enforce one or more neural networks to encode two samples belonging to the same category close in the embedding space, and two samples belonging to a different category far apart in the embedding space. To do so, the triplet loss makes use of three main components:
\begin{itemize}
    \item An anchor sample.
    \item A positive sample belonging to the same category as the anchor.
    \item A negative sample belonging to a category different than the anchor.
\end{itemize}

Based on the components, we can formalize the triplet loss as:

$$L = max(s(a,n) - s(a,p) + margin, 0)$$

Where the \textit{margin} is number that enforces the score between the anchor and the positive sample to be higher compared to the anchor and the negative sample by the \textit{margin}. The scoring function \textit{s(x,y)} can be any function that returns the similarity or the score between two vectors. In this case, $s(i,s)$ is a scoring function that computes the similarity between the image embedding $i$ and the sentence embedding $s$. The scoring function used in this thesis is the \textit{L2} normalized dot-product similarity, which is the equivalent to the cosine similarity.

$$s(i,s) = \frac{i \cdot s}{\vert \vert i \vert \vert^2 \cdot \vert \vert s \vert \vert^2}$$

According to the triplet loss function, we can define 3 types of triplets:
\begin{itemize}
    \item \textbf{Easy triplets}: Triplets where the loss \textit{L} is 0 because: $s(a,p) > s(a,n) + margin$
    \item \textbf{Semi-hard triplets}: Triplets where the loss \textit{L} is positive because of the influence of the margin: $s(a,n) < s(a,p) < s(a,n) + margin$
    \item \textbf{Hard triplets}: Triplets where the loss \textit{L} is positive even if we exclude the margin: $s(a,p) < s(a,n)$
\end{itemize}

Figure \ref{fig:hard_negatives} illustrates the three types of negative samples given an \textit{anchor} and a \textit{positive} sample. 

\begin{figure}
  \centering
  \includegraphics[width=60mm]{Images/hard_negatives.png}
  \caption[The types of negatives in a triplet]{Given an anchor sample and a positive samples, three types of negatives samples can be used to form the triplet. The easy negatives, for which the loss is 0 by default, the semi-hard negatives, for which the loss is positive, but the margin is not violated, and the hard negatives that violate the margin.}
  \label{fig:hard_negatives}
\end{figure}

\subsection{Offline and online triplet mining}
The first challenge that occurs when using the \textit{triplet loss} is coming up with the triplets or the triplet mining. One way to mine the triplets would be to compute them at the start of a training epoch in an \textit{offline mining} way. However, the main problem with this approach is that it artificially increases the size of the training set by including a lot of redundant samples. Consequently, each of the samples in the triplets would have to be forward-propagated through the network to obtain their embedding. Because of the nature of the offline triplet mining, many of the triplets would contain the same samples over and over again. In other words, if the batch size is $B$, the computational complexity would increase by a magnitude of $3B$. This results in an inefficient algorithm that is unnecessary slow. An additional issue with this approach is that there is no control over the type if triplets that are going to be produced.\endgraf

The \textit{online mining} approach creates the triplets on the fly \cite{schroff2015facenet}. In image-text matching, a batch of pairs of matching images and sentences is propagated through the neural network, and their corresponding embeddings are produced. Then, in each pair, the image embedding or the sentence embedding interchangeably take the role of the anchor sample and the positive sample. The negative sample, however, is chosen as one of the other instance embeddings in the batch. That would result in $2B$ instance embeddings produced, which eliminates the redundant computation present in the offline mining approach.

\subsection{Techniques for selecting negative samples}
With the online mining method, we end up with $2B$ embeddings that correspond to a batch with $B$ image-sentence pairs that match. On the other hand, the negatives are spread throughout the batch, and for each anchor image and a positive sentence, we have $B - 1$ negative sentences. Consequently, for each anchor sentence and a positive image, we have $B - 1$ negative sentences. In that sense, there are two widely used strategies to select the negative samples, namely \textit{batch-all} and \textit{batch-hard} strategy.

\subsubsection{Batch-all mining of negatives samples}
In the \textit{batch-all} strategy for mining the negative samples the underlying concept is that for each anchor image and a positive sentence, the loss is summed over all negative sentences within the batch. Consequently, for each anchor sentence and a positive image, the loss is summed over all negative images within the batch. Based on the premise, the loss that follows is:

$$L(i,t) = \sum_{\hat{t}}(s(i,\hat{t}) - s(i,t) + m) + \sum_{\hat{i}}(s(\hat{i},t) - s(i,t) + m)$$

The specific thing about the \textit{batch-all} loss is that the summation is done over both the semi-hard and the hard-negatives. Therefore, \citet{faghri2017vse++} give a name to the triplet loss with \textit{batch-all} strategy as \textit{sum of hinges loss}.


\subsubsection{Batch-hard mining of negatives samples}
Inspired by \citet{hermans2017defense}, \citet{faghri2017vse++} make use of the \textit{batch-hard} strategy to mine the negative samples. The motivation for doing so is that the success of the \textit{Recall@1} metric is dependent on the hardest negative in the batch. Namely, that is the instance that is going to be ranked \textit{1st} by the model when doing the cross-modal retrieval. As a result, \citet{faghri2017vse++} redefine the \textit{sum of hinges loss} as:

$$L(i,t) = max_{\hat{t}}(s(i,\hat{t}) - s(i,t) + m)
+ max_{\hat{i}}(s(\hat{i},t) - s(i,t) + m)$$

The main difference between the \textit{sum of hinges loss} and the \textit{max of hinges loss} is that with the \textit{max of hinges loss}, the hardest negative for each positive pair takes all the gradients, wherein the \textit{sum of hinges loss} the gradients are computed over all the negatives with a batch \cite{faghri2017vse++}. Even though both \citet{faghri2017vse++} and \citet{lee2018stacked} report significant improvements by training only on the hardest negatives within a batch, I found that to be extremely unstable. Therefore, the experiments performed in this thesis are using the \textit{batch all strategy} to select the negative samples within a batch.

\section{Attention weights diversification}
The apparent pitfall with using multiple hops of attention on the same vector sequence is the lack of diversity that can occur between the different hops. In other words, in the surface of the loss function, shallow minima may occur where all attention hops attend on the same vector, and all but one attention hops become redundant. Therefore, I include a penalization term that enforces diversity between the attention weights across different attention hops \cite{lin2017structured}.\endgraf
Ideally, the end goal is each attention hop to focus on attending to the least possible number of distinct entities present in the image or in the sentence. Therefore, the \textit{attention weights diversification} term is computed as:

$$L_{div} = \sum_i^M \sum_j^M \vert \vert A_iA_j^T - I \vert \vert^2$$

Where $M$ is the number of attention hops, $A_i$ and $A_j$ are two distinct attention hops, and $I$ is the identity matrix. In addition, because all of the attention weights are normalized probabilities, they satisfy the constraint:

$$\sum_i^N a_i = 1$$

Where $N$ is the number of vectors that the model attends on. Because of that, the dot product between two distinct attention hops must satisfy the constraint:

$$0 =< a_i^T a_j =< 1$$

Where in the most extreme case, when both attention hops attend on the same vector, and the attention weights $a_i$ and $a_j$ are equivalent, the dot product is $1$. However, if we consider the opposite case where both attention hops attend on a unique vector, the dot product is $0$, which is the desired outcome.\endgraf
Moreover, the elements on the main diagonal of $A_i A_j^T$ are the values when $a_i = a_j$, and they comply with the above-mentioned constraints as well. In this case, the desired outcome is each of the attention hops to specialize in attending to the least possible number of vectors. In other words, in case every single attention hop places its weight on exactly one vector, the dot product is $1$. To balance for that, I subtract the identity matrix from the equation to push the attention weights diversification term towards \textit{0} when the attention hops are attending to a number of vectors close to \textit{1}. Finally, the \textit{Euclidean} norm is computed and $L_{div}$ is multiplied by importance constant prior to contributing to the total model loss.