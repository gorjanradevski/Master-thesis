\chapter{The model}
\label{The model}
In this section, I go through the \textit{Siamese multi-hop attention} model in a bottom-up fashion. I start at the lower end of the model, where the raw images and the corresponding sentences enter the model and are met by the data preprocessing and augmentation modules. At the second level of the model architecture are both of the modality encoders, namely the image encoder which is deep convolutional neural network \cite{he2016deep} pretrained on \textit{ImageNet} \cite{deng2009imagenet}, and a sentence encoder which is a language model \cite{peters2018deep} pretrained on the 1 Billion word benchmark dataset \cite{chelba2013one}. Going forward is the attention block where both the outputs from the modality encoders are fed in. The second to last part is the loss computation block where a series of different regularization methods are implemented in addition to the standard matching loss in order to compute the final loss that the model will be trained on. Lastly, the loss is fed into the optimizer block where the gradients are computed and backpropagated through the network \cite{rumelhart1985learning} to update the network weights.

\begin{figure}
  \centering
  \includegraphics[width=70mm]{Images/full_model.png}
  \caption{The full model architecture}
  \label{fig:full_model}
\end{figure}

\section{Data processing}
All of the datasets (with no particular order \cite{rashtchian2010collecting, hodosh2013framing, young2014image}) used to conduct the research in this thesis contain pairs of images and sentences that match one another. Even though the datasets are rather small as per the amount of data that the deep neural nets need to be trained, processing them all at once is not feasible. Because of that, a sequential process has to be implemented where batches of image-sentence pairs are read from the disk, processed, and passed further down the model, so that the training and inference processes can run efficiently. This section in particular sheds light on the processing of the data batches, before they are fed into the model.
\subsection{Image augmentation}
Because the image encoder is \textit{ResNet152} \cite{he2016deep} pretrained on \textit{ImageNet} \cite{deng2009imagenet}, the images are preprocessed in the same way as in \cite{simonyan2014very}. The is due to the fact that the \textit{ResNet152} employs the preprocessing from \textit{VGG16/19}. Because the preprocessing done in \cite{simonyan2014very} comes down to subtracting the \textit{ImageNet} means, the preprocessing here has to be done in the same fashion in order to exclude the chance for any unexpected behaviours. Further more, data augmentation is a series of techniques usually employed when the training data is scarse. This is done in order to include artificial noise in the data and reduce the risk of the model overfitting on the training data. In the standard supervised learning setting $X \rightarrow Y$, the data augmentation techniques are applied on the \textit{X} side, where in each training batch the data is distorted using a series of random transformations. The series of random transformations applied during training the \textit{SMHA}, illustrated on figure \ref{fig:image_augmentation} are:
\begin{itemize}
  \item Random 224x224 centered crop of the image.
  \item Random horizontal flip with 50\% probability.
\end{itemize}
\begin{figure}
  \centering
  \includegraphics[width=100mm]{Images/data_augmentation.png}
  \caption{Image augmentation}
  \label{fig:image_augmentation}
\end{figure}
On the other hand, when performing inference, having noisy data points will be counterproductive. Because of that, in that case, the only transformation done is re-sizing the image to have 224 pixel width and height.
\subsection{Text preprocessing}
In order for the sentence encoder to extract meaningful representations for each of the words in the sentence, the sentences have to cleaned and tokenized. In the cleaning phase, prior to tokenizing the sentence the following transformation are applied to the sentences:
\begin{itemize}
  \item All words in the sentences are lower cased.
  \item All punctuation is removed from the sentences.
\end{itemize}
By doing this, I am ensuring that words which are essentially the same will not be treated as separate words in the vocabulary space. 
\section{Modality encoders}
In this section, I do a walkthrough of both of the modality encoders, namely the image encoder and the sentence encoder.  Since both of the modality encoders heavily rely on transfer learning, in section \ref{sec: transfer} there is a brief introduction of transfer learning which is followed by an explanation of the image encoder in section \ref{sec: image_encoder} and an explanation of the sentence encoder in section \ref{sec: sentence_encoder}.
\subsection{Transfer learning}\label{sec: transfer}
Due to the nature of the cross-modal retrieval datasets, in the scope of this thesis I make heavy use of transfer learning. Compared to other approaches where transfer learning is leveraged on the image encoder branch, here I make use of transfer learning on both the image and text branches to boost the performance further.\endgraf
Transfer learning is the scenario where we are taking a model that has been trained on a task \textit{A}, involving a feature space $X_A$ and a label space $Y_A$, and we are reusing the trained weights from the model to perform on a task \textit{B} with feature space $X_B$ and label space $Y_B$ where $A \neq B$. By doing so, the model that is supposed to perform well on the task \textit{B}, can leverage the knowledge from the model trained on task \textit{A}. Moreover, transfer learning is particularly useful when the domains of the task \textit{A} and task \textit{B} are similar.

\subsection{Image encoder}\label{sec: image_encoder}
The image encoder employed within the model architecture is a deep convolutional neural network \textit{ResNet152}\cite{he2016deep} pretrained on \textit{ImageNet}\cite{deng2009imagenet} and a fully connected layer on top of the \textit{ResNet152} to project the image embedding in the joint latent space. The family of \textit{ResNets} emerged as a way to overcome the degradation problem that occurs when training deeper networks\cite{he2015convolutional}. The degradation problem occurs when, by stacking multiple layers on a neural network, the training accuracy saturates and then decreases rapidly. However, since the accuracy is computed on the training set, this phenomenon cannot be related to overfitting, which is the case where the neural network's performance increases on the training set but decreases on the validation set. The \textit{degradation} phenomenon has been investigated and empirically proven by \cite{he2016deep, he2015convolutional}. Therefore, to overcome the degradation problem, in \cite{he2015convolutional} the residual block is introduced. The main purpose of the residual block is to sidestep learning an explicit mapping between a block of stacked layers, and instead let the stacked layers learn a residual mapping. In order to do so, the original mapping is recast into:

$$F(x) + x$$

by letting the the stacked layers fit the mapping:

$$F(x) = H(x) - x$$

where $H(x)$ is the desired mapping that we originally wanted to learn.
The motivation for doing so is to bypass the \textit{degradation} problem as explained in \cite{he2015convolutional}. Namely, if a neural network with $N$ layers can sufficiently approximate a desired function $F(x)$, then a neural network with $N + M$ layers where the newly added stack of layers are constructed as identity mappings, should approximate the same function. Based on this hypothesis, it follows that the newly added stack of layers has difficulties approximating the identity function. Therefore, if the identity mapping between the layers is desired, the neural network can successfully learn to push its weights towards zero and thus fit the identity mapping.\endgraf
The building block of the \textit{ResNet} family of neural networks is:

$$y = W_2 \sigma(W_1x) + x$$

Where $W_1$ and $W_2$ are the weight matrices to be learned, $x$ are the inputs and $\sigma$ denotes the \textit{ReLU}\cite{nair2010rectified} activation function.\endgraf


During training, the \textit{ResNet152} is kept fixed, while a feature vector from the image is extracted from the last convolutional layer before the logits. That results in a $7\times7\times2048$ dimensional representation of the encoded image. Namely, the activation map of the last convolutional layer of the \textit{ResNet152} results in an output of 2048 filters with a width and height of 7.
The motivation for doing such transfer learning\cite{yosinski2014transferable} is that, it has been empirically proven when a convolutional neural network is trained on a dataset of images, the first layer of the neural network learns features that resembles a Gabor filters or color blobs\cite{yosinski2014transferable} which are easily transferable from one image domain to another. Since the learned features at the first few layers of the neural networks are invariant of the objective function and the dataset, these features are considered as general and can be reused on different tasks. On the other hand, the layers that reside at the end of the neural networks tend to specialize on the training objective. Consequently, the typical transfer learning scenario is to obtain the first few layers of a neural network trained on a bigger dataset, and to append these layers on a new neural network, that will be trained on a specific task. The new, task-specific neural network will include other layers, which will be randomly initialized and trained to make use of the extracted features and map them to its corresponding task. In this particular case, the new neural network is exactly one fully connected layer that projects the \textit{ResNet152} features.\endgraf
Figure \ref{fig:image_encoder} illustrates the image encoder branch of the model. On the lower end, the augmented image is provided as input and the \textit{ResNet152} extracts the image features $(S_1, S_2, S_3,...,S_n)$. Each $S_i$ is of size \textit{2048} and holds a single activation pixel from all of the \textit{2048} filters from the \textit{ResNet152} and \textit{n = 49} which corresponds to the flattened activation map size. After projecting number of filters in the joint space through the fully connected layer, the final output from the image encoder is a list of hidden states $(H_1, H_2, H_3,...,H_n)$ where \textit{n} remains equal to 49 and the size of each $H_i$ corresponds to the size of the joint space.

\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/image_encoder.png}
  \caption{The image encoder branch}
  \label{fig:image_encoder}
\end{figure}


% SENTENCE ENCODER%


\subsection{Sentence encoder}\label{sec: sentence_encoder}
The input to the sentence encoder is a sequence of $N$ word tokens $(w_1, w_2,...,w_N)$. The commonly adopted way to encode the word tokens in hidden representations is to initially encode each word $w_t$ as a one-hot vector $[0,0,...,1,0,0]$ where the vector index that is $1$, corresponds to the word index $t$. Afterwards, each of the word vectors are embedded into $M$ dimensional vectors through a word embedding matrix $W_e^Tw_t$ \cite{nam2017dual, kiros2014unifying, wang2018learning, lee2018stacked, faghri2017vse++}. In this thesis, I argue that such an approach is undesired due to the lack of training data in the commonly used benchmark datasets. In order for the word embedding matrix to learn sufficiently good vector representations of words, it needs to be trained on much bigger datasets \cite{mikolov2013distributed, pennington2014glove, peters2018deep}. Moreover, I argue that intializing the word embedding matrix with a fixed pretrained word embeddings and then extracting a sentence meaning from this matrix\cite{klein2015associating} is undesired. This is the case because, in the problem of cross-modal retrieval, the text is a weak annotation of the image. In that sense, having fixed word embeddings limits the ability of the model to adjust the meaning of the word based on both the word context and matching image.
Therefore, instead of training the word embedding matrix from scratch, or initializing it with fixed word reprensetations I make use of a word embeddings from a language model, or also known as \textit{ELMo} \cite{peters2018deep}. Then, a higher level context depended representations of the words are learned with a \textit{GRU} recurrent neural network \cite{cho2014learning} on top of the \textit{ELMo}.\endgraf

The \textit{ELMo} word embeddings, are word embeddings obtained from the internal states of a bidirectonal langauge model. Given a sequence of $N$ word tokens $(w_1, w_2,...,w_N)$, a forward language model computes the probability of the whole sequence by computing the probability of a word token $w_t$, given its history $(w_1, w_2,...,w_{t-1})$:
$$p(w_1, w_2,...,w_N) = \prod_{t=1}^N p(w_t \vert w_1,w_2,...,w_{t-1})$$
On the other hand, a backward language model shares the same characteristics with a forward language model, except that it computes everything in reverse. In other words, it will compute the probability of the whole sequence by computing the probability of a word token $w_t$, given its future $(w_{t+1},w_{t+2},...,w_{N})$:
$$p(w_1, w_2,...,w_N) = \prod_{t=1}^N p(w_t \vert w_{t+1},w_{t+2},...,w_N)$$
\endgraf
The bidirectional language model used to compute \textit{ELMo}, is based on the recent state of the art language models \cite{melis2017state, jozefowicz2016exploring} where initially a context independent representation for each of the tokens is obtained using character convolutions. The obtained representation is then passed through \textit{L} layers of forward and backward \textit{LSTMs}\cite{hochreiter1997long}. In the model later used to compute \textit{ELMo}, during training, at time $t$, the \textit{LSTM} is presented with a context independent representation of the word token $w_t$. In order to convert the context independent representation of the word token $w_t$ into a context dependent one, the \textit{LSTM} goes through a multi step process that is managed by it's gates. The \textit{LSTM} has 3 of these gates. The first of the \textit{LSTM} gates decides how much of the past information is going to be forgotten:
$$f_t = \sigma(W_f \cdot [h_{t-1}, w_t] + b_f)$$
Because of the role that this gate plays, it is called the forget layer gate. The second step is to decide how much of the new information $w_t$ is going to be stored in the cell:
$$i_t = \sigma(W_i \cdot [h_{t-1}, w_t] + b_i)$$
$$\hat{C_t} = tanh(W_c \cdot [h_{t-1}, w_t] + b_c)$$
$$C_t = f_t * C_{t-1} + i_t * \hat{C_t}$$
Consequently, this gate is called the input layer gate because it updates memory of the cell based on the present information. Finally, the output layer gate is responsible for providing the context dependent representation for the word token $w_t$:
$$o_t = \sigma(W_o \cdot [h_{t-1}, w_t] + b_o)$$
$$h_t = o_t * tanh(C_t)$$
Here, 2 layers $(L = 2)$ of forward and backward \textit{LSTMs} are stacked on top of each other followed by a softmax that will return the probability for the next word in the sequence.\endgraf
In this thesis, in order to come up with \textit{ELMo} embeddings for each of the words tokens, the softmax layer is stripped off, and a weighted average of the internal states of the pretrained bidirectional language model\cite{peters2018deep} is computed. Assuming that we are given a sequence of $N$ tokens, then the \textit{ELMo} embedding of the sequence of tokens would be a tensor $E = (e_1^{elmo}, e_2^{elmo},...,e_N^{elmo})$ where each $e_t^{elmo}$ is a 1024 dimensional vector. However, the sequence representations $E$ still resides in the \textit{Bidirectional language model} latent space and the end goal is to have the text embedding projected in a joint latent space together with the image encoding. In order to do so, I add a multilayered bidirectional \textit{GRU}\cite{cho2014learning} on top. The purpose for having such a recurrent neural network on top of the \textit{ELMo} embeddings is besides projecting the sentence embedding in the image encoder space, to extract higher level context dependent representations for each of the words. In the \textit{image encoder} case for example, this was not needed and a simple fully connected layer is used to do the projection.\endgraf
The \textit{GRU} recurrent neural network, is a simplified version of the \textit{LSTM} \cite{hochreiter1997long} neural network, that fuses together the input and update layer gates into a single update gate. Consequently, given a sequence of past \textit{ELMo} word embeddings $E = (e_1^{elmo}, e_2^{elmo},...,e_{t-1}^{elmo})$, the hidden state of the forward $\overrightarrow{GRU}$ recurrent neural network, will be computed as result of the equations:


$$z_t = \sigma(W_z \cdot [h_{t-1}, e_t])$$
$$r_t = \sigma(W_r \cdot [h_{t-1}, e_t])$$
$$\hat{h_t} = tanh(W \cdot [r_t * h_{t-1}, e_t])$$
$$h_t = (1 - z_t) * h_{t-1} + z_t * \hat{h_t}$$

On the other hand, the backward $\overleftarrow{GRU}$ will obey to the same equations, with the only difference being that instead of considering the past hidden states, the hidden state $h_t$ will be conditioned on the future hidden states $\overleftarrow{H} = (h_N, h_{N-1},...,h_{t+1})$. Finally, in order to get the feature embedding for the whole text, I average together the forward and backward sequences of hidden states:

$$H = \frac{\overrightarrow{H} + \overleftarrow{H}}{2}$$


\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/sentence_encoder.png}
  \caption{The sentence encoder branch}
  \label{fig:sentence_encoder}
\end{figure}

On figure \ref{fig:sentence_encoder} the whole sentence encoder branch of the model is portrayed. The sequence of words $(w_1, w_2, w_3,...,w_n)$ are entering the \textit{Bidirectional language model} used to compute the \textit{ELMo} embeddings. The output $(E_1, E_2, E_3,...,E_n)$ are the \textit{ELMo} embeddings for each of the words in the sequence. After, each of \textit{ELMo} embeddings are passed through a bidirectional GRU recurrent neural network. Therefore, each of the final hidden states $(H_1, H_2, H_3,...,H_n)$ are the average of the forward and backward GRU cell where \textit{n} is the length of the sequence and the size of each $H_i$ correponds do the size of the joint space.

% ATTENTION %


\section{Attention block}
The attention mechanism, initially introduced by \cite{bahdanau2014neural}, is inspired by the human ability to pay attention to visual parts of an image or correlate words in a sequence\cite{rensink2000dynamic, corbetta2002control}. Firstly applied in the sequence-to-sequence translation with recurrent neural networks\cite{sutskever2014sequence} to overcome the encoder bottleneck, it was immediately extended to visual caption generation\cite{xu2015show} as well as document classification \cite{yang2016hierarchical}. A recently introduced variant of the attention mechanism is the self-attention \cite{cheng2016long} mechanism and it's extensions \cite{lin2017structured}. In this thesis, I am using a variant of \cite{lin2017structured, xu2015show} in order to attend multiple times at separate parts of both the image and the sentence. Moreover, I have empirically found that in case the weights of the attention block are reused across the modalities, the model achieves at least as good performance as the one that uses distinct attention weights for both of the modalities. Because such reusing of weights is firstly introduced by \cite{bromley1994signature} and their \textit{Siamese neural network}, I call this attention mechanism the \textit{Siamese multi-hop attention} or shortly \textit{SMHA}.\endgraf
\textit{SMHA} tries to leverage the fact that multiple entities are present within a sentence and an image, and for a model to be able to do the image-text and text-image retrieval successfully it has to attend multiple times on both modalities. By doing that, the outputs are attention weighed vectors where each vector encodes information about a distinct part of the image or the text. The \textit{SMHA} block is applied on top of both the image encoder and the text encoder and as such, it extracts these vectors for both modalities.\endgraf
Suppose that the output of one of the encoders consist of $N$ hidden representations $H = (H_1, H_2,H_3,...,H_N)$. Consequently, the dimensions of $H$ would be $(N, D)$ where $N$ are the number of hidden representations and $D$ is the dimensionality of the joint hidden space. However, in this case we are only ensured that \textit{D} is going to be the same for both of the outputs of the modality encoders. On the other hand, \textit{N} which is the number of tokens in the \textit{sentence encoder} case, and the spatial size in the \textit{image encoder} case, can vary from one modality to another. Therefore, in order to compute the similarity between the image embedding and the sentence embedding both of the modalities need to be encoded with a fixed size vector. An apparent method to overcome this issue is to perform a pooling on the hidden states from each of the modalities. However, when doing such pooling, all of the hidden states are going to contribute equally in the final embedding from each of the modalities which is undesirable. Therefore, the attention mechanism is a way to compute the final representation as a linear combination of the all hidden states. The atteniton mechanism takes the hidden states as inputs, and outputs a set of weights for each of the hidden states. The weights are computed using a one layer feed-forward neural network with a \textit{tanh} activation function. 

$$U = tanh(W_u H + b_u)$$
$$A_{t} = \frac{e^{U^T W_a}}{\sum_t e^{U^T W_a}}$$

In the equations above, $W_u$ is the weight matrix of the hidden layer and $W_a$ is the weight matrix of the output layer of the attention mechanism. The size of $W_u$ and $W_a$ are chosen as the values that yield the best score on the validation set. 

\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/attention_weights.png}
  \caption{Obtaining the attention weights}
  \label{fig:attention_weights}
\end{figure}

Figure \ref{fig:attention_weights} illustrates the process of coming up with the attention weights. Firstly the hidden representations $H$ are fed through a one layer multi-layered perceptron in order to extract a one level deeper hidden representation $U$. Then, this is followed by a softmax layer where the softmax operation is performed along the second dimension of the input.  The output of the softmax layer are the normalized attention weight for each of the attention hops.\endgraf
Once the attention weights are computed and normalized, the hidden states are weighted accordingly for each attention hop. 

$$O = A \cdot U$$

Figure \ref{fig:attention_attend} illustrates the weighting process. The matrix $O$ is the attended embedding matrix of the $H$ hidden representation, where each of the rows of the matrix are the outputs of a single attention hop.\endgraf

\begin{figure}
  \centering
  \includegraphics[width=45mm]{Images/attention_attend.png}
  \caption{Attending on the hidden states}
  \label{fig:attention_attend}
\end{figure}


In the case where the input to the attention block are the outputs from the image encoder, the attention weights are computed along the spatial dimension which is the $width \times height$ of the activation map of the last convolutional layer of the \textit{ResNet152}. On the other hand, if the input the to the attention block is the output from the sentence encoder, the attention weights are computed across the time dimension.\endgraf
Lastly, the the output of the attention block is normalized to lie on the unit hypersphere by computing:

$$O_{normalized} = O / \sqrt{max(\sum_i o_i^2,  \epsilon))}$$

Where $\epsilon$ is a small number to ensure that the element will be positive so that the square root can be defined in that domain.

\section{Matching loss}
In the supervised learning paradigm, namely classification, usually what we have is a fixed number of categories to which each of the instances of the training set belong. Namely, we are learning a mapping $X \rightarrow Y$ from a domain $X$ to a domain of target categories $Y$. Therefore, we can train a classifier that will minimize a loss function that indicates the mismatch between the target output $Y$ and the predicted output. However, there are cases where the number of categories is huge or in some cases even infinite. In those cases, we need to train a classifier that can recognize whether two instances belong to the same category or not.\endgraf
The triplet loss function, introduced by \cite{weinberger2009distance} is a way to train a classifier when the number of distinct categories can vary in the domain $[2, \infty]$. The goal of the triplet loss is to enforce one or more neural networks to encode two samples belonging to the same category close in the embedding space, and two sample belonging to a different category far apart in the embedding space. Therefore, in order to do so, the triplet loss employs three main components:
\begin{itemize}
    \item An anchor sample.
    \item A positive sample belonging to the same category as the anchor.
    \item A negative sample belonging to a category different than the anchor.
\end{itemize}

Based on the components, we can formalize the triplet loss as:

$$L = max(s(a,n) - s(a,p) + margin, 0.0)$$

Where the \textit{margin} is number that enforces the score between the anchor and the positive sample, to be higher than the score between the anchor and the negative sample by that number. The scoring function can be any function that returns the similarity between two vectors. In this case, $s(i,s)$ is a scoring function that computes the similarity between the image embedding $i$ and the sentence embedding $s$. The scoring function used in this thesis is the \textit{L2} normalized dot-product similarity which is the equivalent to the cosine similarity.

$$s(i,s) = \frac{i}{\vert \vert i \vert \vert^2} \cdot 
\frac{s}{\vert \vert s \vert \vert^2}$$

According to the loss function, we can define 3 types of triplets:
\begin{itemize}
    \item \textbf{Easy triplets}: Triplets where the loss is 0 because: $s(a,p) > s(a,n) + margin$
    \item \textbf{Semi-hard triplets}: Triplets where the loss is positive because of the influence of the margin: $s(a,n) < s(a,p) < s(a,n) + margin$
    \item \textbf{Hard triplets}: Triplets where the loss is positive even if we exclude the margin: $s(a,p) < s(a,n)$
\end{itemize}

Figure \ref{fig:hard_negatives} illustrates the three types of negative samples given an \textit{anchor} and a \textit{positive} sample. 

\begin{figure}
  \centering
  \includegraphics[width=60mm]{Images/hard_negatives.png}
  \caption{Easy, semi-hard and hard negatives}
  \label{fig:hard_negatives}
\end{figure}

\subsection{Offline vs online triplet mining}
The first challenge that occurs when using the \textit{triplet loss} coming up with the triplets. One way to mine the triplets would be to compute them at the start of a training epoch in an \textit{offline mining} way. However, the main problem with this approach is that it artificially increases the size of the training set with many redundant samples. Consequently, each of the triplets would have to be forward propagated through the neural network where many of the triplets would contain the same samples over and over again. In other words, if the batch size is $B$, the computational complexity would increase by a magnitude of $3B$. This results in an inneficient algorithm that is unnecessary slow. Moreover, with this approach, we have no control on the type if triplets that are going to be produced.\endgraf
On the other hand, the \textit{online mining} approach creates the triplets on the fly \cite{schroff2015facenet}. In the cross-modal retrieval case, a batch of pairs of matching images and sentences will propagated through the neural network in order to extract the embeddings for each image and sentence in the batch. Then, in each pair, the image or the sentence and interchangeably take the role of the anchor sample and the positive sample and the negative sample can be chosen within the batch. That would result in $2B$ instances propagated through the network, which eliminates the redundant computation present in the offline mining approach.

\subsection{Techniques for online mining the negative samples}
With the online mining method, we end up with $2B$ embeddings that correspond to a batch with $B$ pairs of an image and a sentence that match. On the other hand, the negatives are spread throughout the batch, and for each anchor image and a positive sentence, we have $B - 1$ negative sentences. Consequently, for each anchor sentence and a positive image, we have $B - 1$ negative sentences. The question that gets raised is what is an effective method pick the negatives. In that sense, there are two widely used strategies to select the negative samples, namely \textit{batch-all} and \textit{batch-hard} strategy.

\subsubsection{Batch-all mining of negatives samples}
In the \textit{batch-all} strategy for mining the negative samples the underling concept is that for each anchor image and a positive sentence, the loss will be summed over all negative sentences within the batch. Consequently, for each anchor sentence, and a positive image, the loss will be summed over all negative images within the batch. Based on the premise the loss that follows is:

$$L(i,t) = \sum_{\hat{t}}(s(i,\hat{t}) - s(i,t) + m) + \sum_{\hat{i}}(s(\hat{i},t) - s(i,t) + m)$$

The specific thing about the \textit{batch-all} loss is that the summation is done over both the semi-hard and the hard-negatives. Therefore, in \cite{faghri2017vse++} the triplet loss with \textit{batch-all} strategy is titled as \textit{sum of hinges loss}.  
\subsubsection{Batch-hard mining of negatives samples}
Inspired by \cite{hermans2017defense}, the authors in \cite{faghri2017vse++} make use of the \textbf{batch-hard} strategy to mine the negative samples. The motivation for doing so is that the success of the \textit{Recall@1} metric is dependent on the hardest negative within the batch.\endgraf
Therefore, based on \cite{faghri2017vse++}, the \textit{sum of hinges loss} can be redefined as:

$$L(i,t) = max_{\hat{t}}(s(i,\hat{t}) - s(i,t) + m)
+ max_{\hat{i}}(s(\hat{i},t) - s(i,t) + m)$$

To form the \textit{max of hinges loss} The main difference between the \textit{sum of hinges loss} and the \textit{max of hinges loss} is that with the \textit{max of hinges loss}, the hardest negative for each positive pair takes all the gradients, where in the \textit{sum of hinges loss} the gradients are computed over all the negatives with a batch \cite{faghri2017vse++}. Even though both the authors of \cite{faghri2017vse++} and \cite{lee2018stacked} report significant improvements by training only on the hardest negatives within a batch, I found that to be extremely unstable. Therefore, the experiments performed within this thesis are using the \textit{batch all strategy} to mine the negative samples.

\section{Attention weights diversification}
The obvious pitfall with using multiple hops of attention on the same sequence of vectors is the lack of diversity that can occur between the different hops. In other words, in the surface of the cost function shallow minimas may occur where all attention hops attend on the same part of the vector, and all but one attention hops become redundant. Therefore, as in \cite{lin2017structured} I include a penalization term that will enforce the attention weights to be diverse.\endgraf
Namely, what we want is each of attention hops to focus on encoding the least possible number of entities that are present within the image or mentioned within the sentence. Moreover, we want each of the attention hops to attend on a distinct part of the image or combination of words in the sentence. In other words, entity that no other attention hop has already attended to. Thus, the \textit{attention weight diversification} term that is added to the loss, and minimized together will the matching loss is:

$$L_{div} = \sum_i^M \sum_j^M \vert \vert A_iA_j^T - I \vert \vert^2$$

Where $M$ are the number of attention hops that are computed, $A_i$ and $A_j$ are two distinct attention hops and $I$ is the identity matrix.\endgraf
Because all of the attention weights for the different attention hops are normalized probabilities, they satisfy the constraint:

$$\sum_i^N a_i = 1$$

Where $N$ is the number of vectors that the model attends to. Consequently, the dot product between two distinct attention hops must satisfy the constraint:

$$0 =< a_i^T a_j =< 1$$

Where in the most extreme case, where both attention hops attend on the same thing the dot product is $1$. However, if we consider the reverse extreme case, where both attention weights attend on a unique entity the dot product is $0$, which is the desired outcome.\endgraf
Moreover, the elements on the main diagonal of $A_i A_j^T$ are the values when $a_i = a_j$. Therefore, they satisfy the above mentioned constraints as well. However, in this case, the desired outcome is that each of the attention hops to specialize in attending to the least possible number of entities. In other words, in case every single attention hop places its weight on exactly one entity the dot product will be $1$. As a result, I subtract the identity matrix from the equation which will enforce the specialization of attention hops. Finally, the Euclidean is computed for each of the attention hops. The $L_{div}$ is multiplied by an importance constant and minimized together with the matching triplet loss.

\section{Optimizer}
Once the total loss is computed the neural network needs to update its weights accordingly. There exist many different techniques to minimize the loss function where most of them are based on gradient descent\cite{ruder2016overview}. Gradient descent, in its purest form, computes the gradient of the cost function with respect to the neural network's parameters:

$$\theta = \theta - \alpha \cdot \nabla_{\theta} L(\theta)$$

Where $\theta$ are the network parameters, $\alpha$ is the learning rate and $L(\theta)$ is the loss function computed with respect to the model's parameters.\endgraf
The main issue with the default variant of gradient descent is that in order for the method to perform one update of the weights of the neural network, firstly the loss has to be computed for the entire training set at once. However, when dealing with large datasets that becomes unfeasible. Therefore, stochastic gradient descent \cite{robbins1951stochastic} is a simplified version of gradient descent, where the update formula becomes:

$$\theta = \theta - \alpha \cdot \nabla_{\theta} L(\theta; i^{z:z+n}; t^{z:z+n})$$

Based on the single weight update formula, stochastic gradient descent performs update on the weights after the loss is computed for a single batch. In the scope of this thesis, after the total loss is computed for a batch of images and sentences $(i^{z:z+n};t^{z:z+n})$, stochastic gradient descent is going to update the weights of the neural network. However, stochastic gradient descent encounters many challenges that need to be overcome in order for the neural network to converge in a sufficiently good minima. Some of these challenges are \cite{ruder2016overview}:

\begin{enumerate}
    \item The method is highly sensitive to the choice of the learning rate $\alpha$. Namely, a too high learning rate my result in never reaching convergence and a too low learning rate may take too much time to converge.
    \item The surface of the loss function is non-convex and has many saddle points where the gradient is $0$. As a result, gradient descent may get stuck in one of these points where no further update can be performed.
    \item Gradient descent uses the same learning rate to update all parameters in the model which is undesirable. Different parameters have different importance and updating them all the same is not a preferred property.
\end{enumerate}

The gradient descent optimizer with momentum \cite{qian1999momentum} is method that overcomes the \textit{2} challenge of stochastic gradient descent. The momentum optimizer update formula is:

$$v_t = \gamma v_{t-1} + \alpha \nabla_{\theta}L(\theta; i^{z:z+n}; t^{z:z+n})$$
$$\theta = \theta - v_t$$

Where a fraction $\gamma$ of the past update vector $v_{t-1}$ is added on the current update vector $v_t$ and $\gamma$ is a parameter that needs to be tuned on the validation set. However, even with the addition of the \textit{momentum term}, the challenges \textit{1} and \textit{3} are still present. In that sense, \textit{Adagrad's} \cite{duchi2011adaptive} update rule, at a time step $t$, for a single model parameter $\theta_i$ is:

$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{G_{t, ii} + \epsilon}} \cdot \nabla_{\theta}L(\theta; i^{z:z+n}; t^{z:z+n})$$

Where $G_t$ is a diagonal matrix that stores the sum of squares of the gradients with respect to the parameter $\theta_i$ up to the time step $t$. The main advantage of \textit{Adagrad} is that the learning rate does not need to be tuned, and it is adapted for each parameter $\theta_i$ based on the past gradients for the parameter. However, \textit{Adagrad} comes together with the pitfall of rapidly decreasing learning rate. Namely, because the elements of $G$ are the sums of the squared gradients, as the training progresses, the learning rate becomes small to the point where the updates are insignificant. To overcome the vanishing learning rate, \textit{RMSprop} \cite{tieleman2012lecture} stores a running average of the magnitudes of past gradients in a fixed window $w$. Therefore, \textit{RMSprop} defines:

$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma)g_t^2$$

where $g$ is the gradient of the loss function:

$$g_t = \nabla_{\theta}L(\theta; i^{z:z+n}; t^{z:z+n}$$

and $\gamma$ is defines the fraction of the past running average that will be considered. Therefore, the update formula for a single parameter $\theta_i$ becomes:

$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t$$

A final enhancement of gradient descent is \textit{Adam} \cite{kingma2014adam}. \textit{Adam} is a method that is built on top of \cite{tieleman2012lecture, duchi2011adaptive} that also keeps a exponentially decaying average of past gradients which is similar to the momentum term in \cite{qian1999momentum}. Therefore, the exponentially decaying average of past gradients like the gradient descent optimizer with \textit{momentum} is:

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t$$

While the exponentially decaying average of past squared gradients like \textit{RMSprop} is:

$$v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$$

Based on the equation defined above, the update rule for a single parameter $\theta_i$ of the \textit{Adam} optimizer becomes:

$$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot m_t$$

In \cite{kingma2014adam} it is empirically proven that \textit{Adam} is guaranteed to perform as least as good as the other simpler variants so it is the go to optimizer in the scope of this thesis.

\section{Weights intialization}
NA KRAJ MOZES I NESTO DA NAPISES ZA OVA
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
