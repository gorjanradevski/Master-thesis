\chapter{Conclusion}
\label{cha:conclusion}
In this thesis, I propose \textit{Siamese multi-hop attention}, a deep learning architecture for image-text matching. My main contribution is extending the work of \citet{lin2017structured} for the image-text matching problem. I carry out a comprehensive empirical analysis to emphasize that using multiple hops of attention is essential for image-text matching. By visualizing the image-text matching, we can conclude that the model learns to attend on the essential parts of the image and the sentence while neglecting the rest. Moreover, I propose using a variant of the multi-hop attention called \textit{siamese multi-hop attention}, where the visual and textual attention have tied weights. In addition to that, I conduct experiments to prove that the less memory-demanding siamese variant is guaranteed to perform at least as good as the one that uses separate weights for the visual and textual attention. Finally, to the best of my knowledge, this work is the first one to leverage transfer learning by using \textit{ELMo} on the sentence encoder branch. An ablation study confirms the superiority of the model that uses \textit{ELMo} compared to the one that trains the word embeddings from scratch.\endgraf
As future work, a good starting point would be to fine-tune the image encoder. As reported by \citet{lee2018stacked, faghri2017vse++}, that is guaranteed to improve \textit{Recall@K} on the image-text and text-image retrieval tasks significantly. However, in the scope of this thesis that was not feasible due to the memory demands of \textit{ResNet152}. Another direction where this work can be extended is the use of a better sentence encoder. Nowadays, the research is moving from the typical recurrent neural network based models \cite{hochreiter1997long, cho2014learning} towards the use of \textit{transformers} \cite{vaswani2017attention}. \endgraf
\begin{comment}
The industrial application of such deep learning architecture is incredibly broad. So far, the industry is flooded with algorithms that can perform information retrieval in the text-text or image-image cases. However, to the best of my knowledge, a cross-modal retrieval algorithm is yet to be applied in a commercial application. Therefore, the \textit{Siamese multi-hop attention} deep learning architecture can operate within any information retrieval system where the queries submitted by the users are textual, and the database entries are images or vice versa.
\end{comment}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
