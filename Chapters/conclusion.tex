\chapter{Conclusion}
\label{cha:conclusion}
In this thesis, I propose \textit{Siamese multi-hop attention}, a deep learning architecture for image-text matching. My main contribution is expanding the work of \citet{lin2017structured} for the image-text matching problem. Even though the \textit{multi-hop attention} is equivalent with the \textit{structured self-attention} of \cite{lin2017structured}, the following discrepancies between the two hold:
\begin{itemize}
    \item \citet{lin2017structured} used their model for sentiment analysis and entailment, while \textit{SMHA} is used for image-text matching.
    \item \textit{Structured self-attention} is used to obtain sentence embedding, followed by a task specific feed-forward neural network with softmax. On the other hand, \textit{SMHA} utilizes \textit{multi-hop attention} where the attention hops are applied directly on the visual-semantic embeddings to emphasize the salient image regions and sentence words.
\end{itemize}

Similarly, there are methods that use visual and semantic single-hop attention for image-text matching \cite{nam2017dual}. However, I carry out a comprehensive empirical analysis to emphasize that using multiple hops of attention is essential for image-text matching. By visualizing the image-text matching, we can conclude that the model learns to attend to the essential parts of the image and the sentence, while neglecting the rest. Moreover, I propose using a variant of the multi-hop attention called \textit{siamese multi-hop attention}, where the visual and textual attention have tied weights. In addition to that, I conduct experiments to prove that the less memory-demanding siamese variant is guaranteed to perform at least as good as the one that uses separate weights for the visual and textual attention. Finally, to the best of my knowledge, this work is the first one to leverage transfer learning by using \textit{ELMo} to improve the sentence embedding. An ablation study confirms the superiority of the model that uses \textit{ELMo} compared to the one that trains the word embeddings from scratch.\endgraf
For future work, a good starting point is to fine-tune the image encoder. As reported by \citet{lee2018stacked, faghri2017vse++}, that is guaranteed to improve \textit{Recall@K} on the image and sentence retrieval tasks significantly. However, in the scope of this thesis that was not feasible due to the memory demands of \textit{ResNet152}. Another direction where this work can be extended is the use of a better sentence encoder. Nowadays, the research is moving from the typical recurrent neural network based models \cite{hochreiter1997long, cho2014learning} towards the use of \textit{transformers} \cite{vaswani2017attention}. \endgraf
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
