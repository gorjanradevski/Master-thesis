\chapter{Conclusion}
\label{cha:conclusion}
In this thesis, I propose \textit{Siamese multi-hop attention}, a deep learning architecture for image-text matching. My main contribution is expanding the work of \citet{lin2017structured} for the image-text matching problem. I carry out a comprehensive empirical analysis to emphasize that using multiple hops of attention is essential for image-text matching. By visualizing the image-text matching, we can conclude that the model learns to attend to the essential parts of the image and the sentence while neglecting the rest. Moreover, I propose using a variant of the multi-hop attention called \textit{Siamese multi-hop attention}, where the visual and textual attention have tied parameters. In addition to that, I conduct experiments to prove that the less memory-demanding siamese variant is guaranteed to perform at least as good as the one that uses separate parameters for the visual and textual attention module. Finally, to the best of my knowledge, this work is the first one to leverage transfer learning by using \textit{ELMo} to improve the sentence embedding. An ablation study confirms the superiority of the model that uses \textit{ELMo} compared to the one that trains the word embeddings from scratch.\endgraf
For future work, a good starting point is to fine-tune the image encoder. As reported by \citet{lee2018stacked, faghri2017vse++}, that is guaranteed to improve \textit{Recall@K} on the image and sentence retrieval tasks significantly. However, in the scope of this thesis that was not feasible due to the memory demands of \textit{ResNet152}. Another direction where this work can be extended is the use of a better sentence encoder. Nowadays, the research is moving from the typical recurrent neural network based models \cite{hochreiter1997long, cho2014learning} towards the use of \textit{transformers} \cite{vaswani2017attention}. \endgraf
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
