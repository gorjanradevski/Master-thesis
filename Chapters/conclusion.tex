\chapter{Conclusion}
\label{cha:conclusion}
In this thesis, I propose \textit{Siamese multi-hop attention}, a deep learning architecture for cross-modal retrieval. My main contribution is extending the work in \cite{lin2017structured} for the cross-modal retrieval problem when the modalities are images and text. I carry out a comprehensive empirical analysis to emphasize that using multiple hops of attention is essential for the cross-modal retrieval case. By using a series of visualization of what the deep learning model attends to when performing cross-modal retrieval, we can conclude that the model learns to attend on the essential parts of the image and the text while neglecting the rest. Moreover, I propose using a variant of the multi-hop attention called \textit{siamese multi-hop attention}, where the visual and textual attention have tied weights. In addition to that, I conduct experiments to prove that the less memory demanding siamese variant is guaranteed to perform at least as good as the one that uses separate weights for the visual and textual attention. Consequently, all experiments conducted in the scope of this thesis are using the \textit{siamese multi-hop attention}. This raises an opportunity to extend this work in a direction where the knowledge from pre-trained attention weights is transferred across modalities to achieve cross-modal transfer learning.\endgraf
The industrial application of such deep learning architecture is incredibly broad. So far, the industry is flooded with algorithms that can perform uni-modal retrieval in the text-text or image-image cases. However, to the best of my knowledge, a cross-modal retrieval algorithm is yet to be applied in a commercial application. Therefore, the \textit{Siamese multi-hop attention} deep learning architecture can operate within any information retrieval system where the queries submitted by the users are textual, and the entries of the database are images or vice versa.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
