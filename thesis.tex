\documentclass[master=mai,masteroption=ecs]{kulemt}
\setup{title={Siamese multi-hop attention for
image-text matching},
  author={Gorjan Radevski},
  promotor={Prof. dr. Marie-Francine Moens},
  assessor={Prof. dr. Tinne Tuytelaars \\
            Guillem Collel Taleda},
  assistant={Guillem Collel Taleda}}
% The following \setup may be removed entirely if no filing card is wanted
\setup{filingcard,
  translatedtitle=Siamese multi-hop aandacht voor beeld-tekst matching,
  udc=621.3,
  shortabstract={In this thesis, the problem of image-text matching is studied. I leverage the fact that usually, multiple entities are present in the modalities, so a model that can focus on different aspects of each modality should overpower simpler models. Moreover, the majority of the approaches that tackle the image-text matching problem are used as a black box, and the correspondence between the matched image and text remains vague. Lastly, the image-text matching datasets are small, and the training data samples are scarce. Because of that, a heavy emphasis is placed on transferring knowledge from models trained on millions of data samples on different tasks. In this thesis, I present \textit{Siamese multi-hop attention}, a deep learning architecture that overcomes the obstacles mentioned above when performing image-text matching. The \textit{Siamese multi-hop attention} model is evaluated on the cross-modal retrieval task, which is the principal method of evaluating image-text matching models. To do so, I perform experiments on a variety of cross-modal retrieval benchmark datasets that include small ones such as \textit{Pascal1k}, \textit{Flickr8k} as well as medium to large ones such as \textit{Flickr30k} and achieve results competitive to the current state-of-the-art. The code developed is made available at: \href{https://github.com/gorjanradevski/SMHA}{https://github.com/gorjanradevski/SMHA}}}
% Uncomment the next line for generating the cover page
%\setup{coverpageonly}
% Uncomment the next \setup to generate only the first pages (e.g., if you
% are a Word user. 
%\setup{frontpagesonly}

% Choose the main text font (e.g., Latin Modern)
\setup{font=lm}

% If you want to include other LaTeX packages, do it here. 
% Float to bind tables to their sections
\usepackage{float}
% Packages to display two images within a figure one next to the other
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{numbers,open={[},close={]},comma}
% Finally the hyperref package is used for pdf files.
% This can be commented out for printed versions.
\usepackage[pdfusetitle,colorlinks,plainpages=false]{hyperref}

%%%%%%%
% The lipsum package is used to generate random text.
% You never need this in a real master's thesis text!
\IfFileExists{lipsum.sty}%
 {\usepackage{lipsum}\setlipsumdefault{11-13}}%
 {\newcommand{\lipsum}[1][11-13]{\par And some text: lipsum ##1.\par}}
%%%%%%%

%\includeonly{chap-n}
\begin{document}

\begin{comment}
\begin{preface}
  I would like to thank everybody who kept me busy the last year,
especially my promoter and my assistants. I would also like to thank the
  jury for reading the text. 
\end{preface}
\end{comment}

\tableofcontents*

\begin{abstract}
In this thesis, the problem of image-text matching is studied. I leverage the fact that usually, multiple entities are present in the modalities, so a model that can focus on different aspects of each modality should overpower simpler models. Moreover, the majority of the approaches that tackle the image-text matching problem are used as a black box, and the correspondence between the matched image and text remains vague. Lastly, the image-text matching datasets are small, and the training data samples are scarce. Because of that, a heavy emphasis is placed on transferring knowledge from models trained on millions of data samples on different tasks. In this thesis, I present \textit{Siamese multi-hop attention}, a deep learning architecture that overcomes the obstacles mentioned above when performing image-text matching. The \textit{Siamese multi-hop attention} model is evaluated on the cross-modal retrieval task, which is the principal method of evaluating image-text matching models. To do so, I perform experiments on a variety of cross-modal retrieval benchmark datasets that include small ones such as \textit{Pascal1k}, \textit{Flickr8k} as well as medium to large ones such as \textit{Flickr30k} and achieve results competitive to the current state-of-the-art. The code developed is made available at: \href{https://github.com/gorjanradevski/SMHA}{https://github.com/gorjanradevski/SMHA} 
\end{abstract}

% A list of figures and tables is optional
\listoffigures
\listoftables
% If you only have a few figures and tables you can use the following instead
% \listoffiguresandtables
% The list of symbols is also optional.
% This list must be created manually, e.g., as follows:
\chapter{List of Abbreviations and Symbols}
\section*{Abbreviations}
\begin{flushleft}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabularx}{\textwidth}{@{}p{20mm}X@{}}
    \textit{SMHA}   & Siamese multi-hop attention \\
    \textit{SOTA}   & State of the art \\
    \textit{LSTM}   & Long-short term memory \\
    \textit{GRU}    & Gated recurrent unit \\
    \textit{Word2vec} &  Word to vector \\
    \textit{ReLU} & Rectified-linear unit \\
    \textit{ELMo} & Embeddings from language model \\
    \textit{tanh} & Hyperbolic function \\
    \textit{ResNet} & Residual network \\
    \textit{VGG} &  Visual Geometry Group's neural network \\
    \textit{CNN} &  Convolutional neural network \\
    \textit{R-CNN} &  Region-convolutional neural network \\
    \textit{AlexNet} & Convolutional neural network designed by Alex Krizhevsky \\
    \textit{max(x,y)} & Maximum of \textit{x} and \textit{y}
  \end{tabularx}
\end{flushleft}
\section*{Symbols}
\begin{flushleft}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabularx}{\textwidth}{@{}p{20mm}X@{}}
    \textit{p}    & Probability \\
    $\prod_{i}^{n}$ & Product from \textit{i} to \textit{n} \\
    $\sum_{i}^{n}$ & Sum from \textit{i} to \textit{n} \\
    $\sigma$ & Activation function \\
    $s$ & Scoring function \\
    $\epsilon$ & A small number
  \end{tabularx}
\end{flushleft}

% Now comes the main text
\mainmatter

\include{Chapters/intro}
\include{Chapters/related_work}
\include{Chapters/the_model}
\include{Chapters/experiments}
\include{Chapters/conclusion}

% If you have appendices:
\appendixpage*          % if wanted
\appendix
\include{Appendix/training}
\include{Appendix/additional_examples}

\backmatter
% The bibliography comes after the appendices.
% You can replace the standard "abbrv" bibliography style by another one.
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
